{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Applications with Vector Databases\n",
    "Retrieval-Augmented Generation (RAG) indeed leverages vector databases to enhance the capabilities of language models by integrating external knowledge. This approach is particularly useful for applications requiring up-to-date information and domain-specific knowledge.\n",
    "\n",
    "Here are some key aspects of state-of-the-art RAG methods:\n",
    "\n",
    "1. **Chunking**: This involves breaking down large documents into smaller, manageable pieces or \"chunks\" that can be individually indexed and retrieved. This improves the efficiency and accuracy of the retrieval process[1](https://www.promptingguide.ai/research/rag).\n",
    "\n",
    "2. **Embedding**: Embeddings are numerical representations of text that capture semantic meaning. By embedding both the query and the chunks, the system can effectively compare and retrieve the most relevant information[1](https://www.promptingguide.ai/research/rag).\n",
    "\n",
    "3. **Metadata Usage**: Incorporating metadata (such as document type, date, and author) can enhance the retrieval process by providing additional context that helps in filtering and ranking the results[1](https://www.promptingguide.ai/research/rag).\n",
    "\n",
    "4. **Multimodal RAG**: This involves using RAG techniques across different types of data, such as text and images. For example, embedding and storing images in a vector database allows for querying images with text, enabling more versatile and comprehensive search capabilities[2](https://github.com/NirDiamant/RAG_TECHNIQUES).\n",
    "\n",
    "\n",
    "[1](https://www.promptingguide.ai/research/rag): [Nextra](https://www.promptingguide.ai/research/rag)\n",
    "[2](https://github.com/NirDiamant/RAG_TECHNIQUES): [GitHub - NirDiamant/RAG_Techniques](https://github.com/NirDiamant/RAG_TECHNIQUES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1: Optimizing RAG\n",
    "\n",
    "#### 1.1 What is a Vector Database?\n",
    "\n",
    "- A vector database is a specialized tool designed to handle unstructured data represented as vectors.\n",
    "- The term \"vector database\" is somewhat misleading; these are not traditional databases but compute engines optimized for vector data.\n",
    "- In Generative AI, vector data is often referred to as vector embeddings. For RAG, vectors and embeddings are used interchangeably.\n",
    "\n",
    "#### 1.2 Understanding Vector Embeddings\n",
    "\n",
    "- Vector embeddings are long series of numbers, typically consisting of hundreds or thousands of values.\n",
    "- Generated by deep neural networks, specifically from the second-to-last layer, encapsulating learned information without the final predictive layer.\n",
    "- Embedding models generate these embeddings and vary widely in architecture and data types they can process.\n",
    "\n",
    "**Key Points:**\n",
    "  - **Data and Model Type Matching**: Match the embedding model to the data type (e.g., image models for images, text models for text).\n",
    "  - **Vector Size Consistency**: Only compare vectors of the same size, using the same embedding model for both vectorization and retrieval.\n",
    "\n",
    "#### 1.3 The Role of Large Language Models (LLMs)\n",
    "\n",
    "- LLMs, such as GPT-4, serve as the interface for interacting with your data in RAG.\n",
    "- Based on the transformer model, LLMs predict the most likely next token given a sequence of tokens.\n",
    "- Publicly available LLMs are trained on extensive datasets of publicly available information, lacking access to your specific data.\n",
    "\n",
    "#### 1.4 How RAG Works\n",
    "\n",
    "1. **Vectorization**: Data is vectorized using embedding models.\n",
    "2. **Storage**: Vectorized data is stored in a vector database.\n",
    "3. **Interaction**: An LLM interfaces with the vector database to retrieve relevant information.\n",
    "\n",
    "**Process:**\n",
    "- A question is vectorized by an embedding model.\n",
    "- The vectorized query searches the vector database for similar embeddings.\n",
    "- Relevant results are retrieved and provided as context to the LLM.\n",
    "- The LLM generates a coherent and contextually appropriate response.\n",
    "\n",
    "#### 1.5 Preprocessing for RAG\n",
    "\n",
    "- Preprocessing involves chunking, embeddings, and metadata.\n",
    "\n",
    "**Components:**\n",
    "  - **Chunking**: Breaking down large text blocks into smaller, manageable chunks.\n",
    "  - **Embeddings**: Vectors generated by embedding models, representing the semantic meaning of input data.\n",
    "  - **Metadata**: Additional data stored alongside embeddings in a vector database, critical for optimizing RAG applications.\n",
    "\n",
    "#### 1.6 Chunking Considerations\n",
    "\n",
    "- Chunking splits documents into smaller, consumable chunks.\n",
    "\n",
    "**Requirements:**\n",
    "  - **Consumable**: Fit within the context window of the embedding model and the LLM.\n",
    "  - **Coherent**: Make sense as standalone pieces of text.\n",
    "  - **Contextual**: Contain all necessary context to answer a question.\n",
    "\n",
    "**Considerations:**\n",
    "  - Chunk size, overlap, and the use of special characters to mark chunk boundaries.\n",
    "\n",
    "#### 1.7 Types of Embeddings\n",
    "\n",
    "- **Dense Embeddings**: Vectors with few zero values, typically created by machine learning models.\n",
    "- **Sparse Embeddings**: Vectors with many zero values, usually generated by algorithms. Dense embeddings are generally preferred for RAG applications.\n",
    "\n",
    "#### 1.8 Metadata in RAG\n",
    "\n",
    "- Metadata includes processing metadata (e.g., section titles, paragraph numbers) and data-related metadata (e.g., publication dates, authors).\n",
    "- Helps filter searches and ensures the LLM can interpret the retrieved data correctly.\n",
    "\n",
    "#### 1.9 Introduction to Embeddings\n",
    "\n",
    "- Before vector embeddings, comparing unstructured data was challenging.\n",
    "- Embedding models, usually deep neural networks, convert various data types (text, images, videos, audio) into vectors or vector embeddings.\n",
    "- Vectors enable quantitative comparison of unstructured data.\n",
    "\n",
    "**Critical Considerations:**\n",
    "  - **Embedding Model**: Choose the correct model for your data type (e.g., ResNet50 for images, Sentence Transformers for text, Whisper for audio).\n",
    "  - **What to Embed**: Focus on embedding text, as it is a crucial medium for AI.\n",
    "  - **How to Compare Embeddings**: Ensure embeddings are of the same size for comparison.\n",
    "\n",
    "**Picking the Right Model:**\n",
    "  - **Embedding Size**: The dimensionality of the vector, affecting computational power needed for comparison.\n",
    "  - **Model Size**: Larger models provide finer results but are more computationally expensive.\n",
    "  - **Training Data**: The dataset used to train the model influences its effectiveness (e.g., language, structure, data size).\n",
    "\n",
    "#### 1.10 Embedding Examples\n",
    "\n",
    "- **Basic Embeddings**: Directly embedding chunks of text.\n",
    "- **Small to Big**: Embedding a sentence but storing the entire paragraph for increased context.\n",
    "- **Big to Small**: Embedding a paragraph but storing individual sentences for post-processing.\n",
    "- **Non-English Embeddings**: Using models trained on non-English data for embedding.\n",
    "\n",
    "#### 1.11 Metadata\n",
    "\n",
    "- Metadata is essential for making vector databases useful, providing context and filtering capabilities.\n",
    "- **Chunking Metadata**: Information from the chunking process (e.g., sentence number, section header).\n",
    "- **Non-Chunking Metadata**: Additional information not tied to chunking (e.g., author, last update).\n",
    "\n",
    "**Storing Metadata:**\n",
    "  - Link to traditional databases or store directly in the vector store for easier and faster access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.12 Chunking\n",
    "\n",
    "Chunking is the process of breaking large texts up into small, workable pieces.\n",
    "\n",
    "In the first block here, three things are imported: the document object, the character text splitter object, and the OS library.\n",
    "\n",
    "1. **Imports**:\n",
    "   - **Document Object**: LangChain's native way to store objects. This is used to add metadata to the text and prepare it for the vector store.\n",
    "   - **Character Text Splitter**: A LangChain object that can split strings based on some preset parameters. In this case, it is used for determining chunk size and chunk overlap.\n",
    "   - **OS Library**: Used for navigating the directory structure of the operating system.\n",
    "\n",
    "2. **Functionality**:\n",
    "   - **Set Folder**: First, ensure that the right folder is being used. In this case, the Big Star Collectibles folder is used. To access the list of text files within this folder, the OS library is used to get a list of the directory.\n",
    "   - **Create List**: Next, create an empty list object to hold all of the chunked up texts that will be created.\n",
    "   - **Loop Through Files**: Then start looping through all of the files and chunking them up. So, what is done in this loop?\n",
    "     - Start by opening up the file and reading the entire page in as a single string.\n",
    "     - Next, create a `CharacterTextSplitter` object. This specific instance is set up to split strings into 128 character chunks with 32 character overlaps.\n",
    "     - Then use the object's `split_text` function and pass the string containing the entire file through to get the chunks.\n",
    "     - The last bit of functionality in the chunking section is to loop through each of these chunked texts and create a document object from each chunk. To ensure that the chunks are kept stored in the vector store, assign it to the `page_content` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "# from langchain.document import Document\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import os\n",
    "\n",
    "# Set the folder containing the text files\n",
    "folder_path = 'Big Star Collectibles'\n",
    "\n",
    "# Get a list of text files in the folder\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "# Initialize an empty list to hold chunked texts\n",
    "chunked_texts = []\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for file_name in file_list:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    # Open and read the entire file as a single string\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Create a CharacterTextSplitter object\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=128, chunk_overlap=32)\n",
    "    \n",
    "    # Split the text into chunks\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    \n",
    "    # Loop through each chunk and create a Document object\n",
    "    for chunk in chunks:\n",
    "        doc = Document(page_content=chunk)\n",
    "        chunked_texts.append(doc)\n",
    "\n",
    "# Print the first chunk to verify\n",
    "print(chunked_texts[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.13 Metadata\n",
    "\n",
    "This section is about storing metadata with chunk data. In LangChain, metadata is stored within the document object. Previously, chunks were stored in the `page_content` parameter. To store metadata, simply add a `metadata` parameter. Metadata is stored as a dictionary, and you can define the metadata to store. Common metadata includes the title of the document and the chunk number, indicating where in the document the chunk was taken from.\n",
    "\n",
    "To implement this, enumerate through the list instead of just looping through it. This allows access to the chunk number and identifies where in the document the chunk was taken from.\n",
    "\n",
    "This code will read all text files in the specified folder, split each file into 128-character chunks with 32-character overlaps, and store each chunk as a `Document` object with metadata in the `chunked_texts` list. The last lines print the first chunk and its metadata to verify the process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from langchain.document import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import os\n",
    "\n",
    "# Set the folder containing the text files\n",
    "folder_path = 'Big Star Collectibles'\n",
    "\n",
    "# Get a list of text files in the folder\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "# Initialize an empty list to hold chunked texts\n",
    "chunked_texts = []\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for file_name in file_list:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    # Open and read the entire file as a single string\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Create a CharacterTextSplitter object\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=128, chunk_overlap=32)\n",
    "    \n",
    "    # Split the text into chunks\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    \n",
    "    # Loop through each chunk and create a Document object with metadata\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        metadata = {\n",
    "            'doc_title': file_name,\n",
    "            'chunk_number': i\n",
    "        }\n",
    "        doc = Document(page_content=chunk, metadata=metadata)\n",
    "        chunked_texts.append(doc)\n",
    "\n",
    "# Print the first chunk and its metadata to verify\n",
    "print(chunked_texts[0].page_content)\n",
    "print(chunked_texts[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.14 Embed and Store\n",
    "\n",
    "With the data chunked and some metadata saved in each of the chunked objects, the next step is to embed the data and store it into a vector database. This involves two new imports: FAISS and HuggingFaceEmbeddings.\n",
    "\n",
    "1. **Imports**:\n",
    "   - **FAISS**: Stands for Facebook AI Similarity Search. This library is the foundation for many popular AI-native vector databases.\n",
    "   - **HuggingFaceEmbeddings**: Imported from `langchain_community`. Initially, LangChain had numerous integrations, but as it grew, many of these integrations moved to the LangChain community library, including HuggingFaceEmbeddings.\n",
    "\n",
    "2. **Embedding and Storing**:\n",
    "   - **Import Libraries**: Start by importing the FAISS library from LangChain and the HuggingFaceEmbeddings from the community module.\n",
    "   - **Instantiate Embedding Function**: Create an instance of the HuggingFaceEmbeddings object. The default embedding model is `all-mpnet-base-v2`, which has 768 dimensions. Only vectors of the same dimensionality can be compared.\n",
    "   - **Create Vector Store**: Use the documents created in the metadata and chunking steps. Pass the embedding function and the documents to create the vector store.\n",
    "\n",
    "This code will read all text files in the specified folder, split each file into 128-character chunks with 32-character overlaps, store each chunk as a `Document` object with metadata, and then embed and store these documents into a FAISS vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import os\n",
    "\n",
    "# Set the folder containing the text files\n",
    "folder_path = 'Big Star Collectibles'\n",
    "\n",
    "# Get a list of text files in the folder\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "# Initialize an empty list to hold chunked texts\n",
    "chunked_texts = []\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for file_name in file_list:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    # Open and read the entire file as a single string\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Create a CharacterTextSplitter object\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=128, chunk_overlap=32)\n",
    "    \n",
    "    # Split the text into chunks\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    \n",
    "    # Loop through each chunk and create a Document object with metadata\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        metadata = {\n",
    "            'doc_title': file_name,\n",
    "            'chunk_number': i\n",
    "        }\n",
    "        doc = Document(page_content=chunk, metadata=metadata)\n",
    "        chunked_texts.append(doc)\n",
    "\n",
    "# Instantiate the HuggingFaceEmbeddings object\n",
    "embedding_function = HuggingFaceEmbeddings()\n",
    "\n",
    "# Create the vector store\n",
    "vector_store = FAISS.from_documents(chunked_texts, embedding_function)\n",
    "\n",
    "# Print the first document's embedding to verify\n",
    "print(vector_store.index[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.15 Querying\n",
    "\n",
    "Querying the vector database is part of what goes on behind the scenes in a RAG application. The LLM queries the `vector_store` to get some context back to create a response. When interacting with the RAG app, this query is not visible. This section provides a peek behind the scenes to see what the LLM sees.\n",
    "\n",
    "When querying a vector database, some top_k results are returned. For LangChain FAISS, the default k is 4. \n",
    "\n",
    "1. **Prepare the Vector Store**:\n",
    "   - **as_retriever Function**: The first step to perform a `vector_store` query in LangChain is to take the `vector_store` and call the `as_retriever` function on it. This prepares the `vector_store` to be queried with strings and abstracts out the necessity of turning a string into an embedding and calling a query function directly.\n",
    "\n",
    "2. **Perform the Query**:\n",
    "   - **invoke Function**: Call the `invoke` function of the retriever and pass a string. The result is the top four results in the `vector_store` according to the embedding model defined earlier.\n",
    "\n",
    "This code will read all text files in the specified folder, split each file into 128-character chunks with 32-character overlaps, store each chunk as a `Document` object with metadata, embed and store these documents into a FAISS vector database, and then query the vector store to retrieve the top 4 results based on the query string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the vector store for querying\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# Perform a query\n",
    "query = \"example query text\"\n",
    "results = retriever.invoke(query)\n",
    "\n",
    "# Print the top 4 results\n",
    "for result in results:\n",
    "    print(result.page_content)\n",
    "    print(result.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.15 Adding the LLM\n",
    "\n",
    "The final part of creating a RAG application on top of the vector store is to add the LLM (Large Language Model). For this part, access to an LLM is needed. This can be done using an API key from providers like OctoAI, OpenAI, or others, or by running an LLM locally. \n",
    "\n",
    "1. **Set Up LLM Access**:\n",
    "   - **Import Environment Variables**: Use Python-dotenv's `load_dotenv` method to load environment variables.\n",
    "   - **Import OpenAI**: Import OpenAI from `langchain_openai` and initialize it as the LLM.\n",
    "\n",
    "2. **Create a Prompt Template**:\n",
    "   - **Prompt Creation**: Create a prompt template for the chat. Use brackets to pass the question and context, similar to using an f-string in Python.\n",
    "   - **ChatPromptTemplate**: Use the `ChatPromptTemplate` object from LangChain to create the prompt template.\n",
    "\n",
    "3. **Create the Chain**:\n",
    "   - **Imports**: Import `RunnablePassthrough` and `StrOutputParser`.\n",
    "     - **RunnablePassthrough**: Takes a string and treats it as a function by passing the string through the function.\n",
    "     - **StrOutputParser**: Parses the output of the chain as a string.\n",
    "   - **Build the Chain**: \n",
    "     - Get the context and the question using the objects mentioned.\n",
    "     - Pass these to the prompt created.\n",
    "     - Pass the prompt to the LLM.\n",
    "     - Pipe the output of the LLM to the `StrOutputParser`.\n",
    "\n",
    "4. **Invoke the Chain**:\n",
    "   - **Get a Response**: Invoke the chain to get a response. The response will combine the queries seen in the query video and return a single string from the given context.\n",
    "\n",
    "This code will read all text files in the specified folder, split each file into 128-character chunks with 32-character overlaps, store each chunk as a `Document` object with metadata, embed and store these documents into a FAISS vector database, and then query the vector store and use the LLM to get a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import RunnablePassthrough, StrOutputParser\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set the folder containing the text files\n",
    "folder_path = 'Big Star Collectibles'\n",
    "\n",
    "# Get a list of text files in the folder\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "# Initialize an empty list to hold chunked texts\n",
    "chunked_texts = []\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for file_name in file_list:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    # Open and read the entire file as a single string\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Create a CharacterTextSplitter object\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=128, chunk_overlap=32)\n",
    "    \n",
    "    # Split the text into chunks\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    \n",
    "    # Loop through each chunk and create a Document object with metadata\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        metadata = {\n",
    "            'doc_title': file_name,\n",
    "            'chunk_number': i\n",
    "        }\n",
    "        doc = Document(page_content=chunk, metadata=metadata)\n",
    "        chunked_texts.append(doc)\n",
    "\n",
    "# Instantiate the HuggingFaceEmbeddings object\n",
    "embedding_function = HuggingFaceEmbeddings()\n",
    "\n",
    "# Create the vector store\n",
    "vector_store = FAISS.from_documents(chunked_texts, embedding_function)\n",
    "\n",
    "# Prepare the vector store for querying\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# Instantiate the OpenAI LLM\n",
    "llm = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Create a prompt template\n",
    "prompt_template = ChatPromptTemplate.from_template(\"Question: {question}\\nContext: {context}\")\n",
    "\n",
    "# Create the chain\n",
    "chain = RunnablePassthrough(\n",
    "    retriever,\n",
    "    prompt_template,\n",
    "    llm,\n",
    "    StrOutputParser()\n",
    ")\n",
    "\n",
    "# Perform a query and get a response\n",
    "query = \"example query text\"\n",
    "response = chain.invoke(query)\n",
    "\n",
    "# Print the response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "How can you modify a Retrieval-Augmented Generation (RAG) application to cite its document sources using metadata stored in a vector database?\n",
    "\n",
    "### Answer\n",
    "To modify a RAG application to cite its document sources, follow these steps:\n",
    "1. **Store Document Metadata**: Ensure that the names of the documents are stored as part of the metadata in the vector store.\n",
    "2. **Retrieve Metadata**: When retrieving objects from the vector store, access this metadata.\n",
    "3. **Prompt Engineering**: Modify the prompt to instruct the language model to cite its sources. This can be done by adding a simple sentence to the prompt, such as \"Cite your sources.\"\n",
    "\n",
    "In this example:\n",
    "- The `VectorStore` class is used to interact with the vector database.\n",
    "- The `retrieve_and_cite` function retrieves documents related to the query and extracts their names from the metadata.\n",
    "- The prompt is modified to include an instruction to cite sources.\n",
    "- The language model generates a response, and the sources are appended to the response.\n",
    "\n",
    "Feel free to modify and expand this code to fit your specific use case! If you have any questions or need further assistance, let me know."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import VectorStore\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Initialize the vector store and language model\n",
    "vector_store = VectorStore()\n",
    "llm = OpenAI()\n",
    "\n",
    "# Function to retrieve documents and cite sources\n",
    "def retrieve_and_cite(query):\n",
    "    # Retrieve documents from the vector store\n",
    "    results = vector_store.similarity_search(query)\n",
    "    \n",
    "    # Extract document names from metadata\n",
    "    sources = [result.metadata['document_name'] for result in results]\n",
    "    \n",
    "    # Create a prompt with the query and instruction to cite sources\n",
    "    prompt = f\"{query}\\n\\nCite your sources.\"\n",
    "    \n",
    "    # Get the response from the language model\n",
    "    response = llm(prompt)\n",
    "    \n",
    "    # Append the sources to the response\n",
    "    cited_response = f\"{response}\\n\\nSources: {', '.join(sources)}\"\n",
    "    \n",
    "    return cited_response\n",
    "\n",
    "# Example usage\n",
    "query = \"Explain the benefits of RAG applications.\"\n",
    "response = retrieve_and_cite(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answer: Cite Your Document Sources\n",
    "To solve the challenge of citing document sources, follow these steps:\n",
    "\n",
    "1. **Store Document Names**: Ensure that the names of the documents are stored in the vector store via document metadata. This allows access to this information when retrieving objects from the vector store.\n",
    "\n",
    "2. **Prompt Engineering**: Add a simple sentence to the prompt to instruct the LLM to cite its sources. This can be done by modifying the prompt template to include a request for source citation.\n",
    "\n",
    "3. **Retrieve and Cite Sources**: When the LLM processes the query, it will include the document titles in its response, indicating where the information was found.\n",
    "\n",
    "Here's an example of how to modify the prompt and retrieve the sources:\n",
    "This code will instruct the LLM to cite its sources by including the document titles in the response. The prompt template is modified to include the sentence \"Please cite your sources,\" ensuring that the LLM provides the necessary citations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cite Your Document Sources\n",
    "# Import necessary libraries\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import RunnablePassthrough, StrOutputParser\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set the folder containing the text files\n",
    "folder_path = 'Big Star Collectibles'\n",
    "\n",
    "# Get a list of text files in the folder\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "# Initialize an empty list to hold chunked texts\n",
    "chunked_texts = []\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for file_name in file_list:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    # Open and read the entire file as a single string\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Create a CharacterTextSplitter object\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=128, chunk_overlap=32)\n",
    "    \n",
    "    # Split the text into chunks\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    \n",
    "    # Loop through each chunk and create a Document object with metadata\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        metadata = {\n",
    "            'doc_title': file_name,\n",
    "            'chunk_number': i\n",
    "        }\n",
    "        doc = Document(page_content=chunk, metadata=metadata)\n",
    "        chunked_texts.append(doc)\n",
    "\n",
    "# Instantiate the HuggingFaceEmbeddings object\n",
    "embedding_function = HuggingFaceEmbeddings()\n",
    "\n",
    "# Create the vector store\n",
    "vector_store = FAISS.from_documents(chunked_texts, embedding_function)\n",
    "\n",
    "# Prepare the vector store for querying\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# Instantiate the OpenAI LLM\n",
    "llm = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Create a prompt template with source citation\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"Question: {question}\\nContext: {context}\\nPlease cite your sources.\"\n",
    ")\n",
    "\n",
    "# Create the chain\n",
    "chain = RunnablePassthrough(\n",
    "    retriever,\n",
    "    prompt_template,\n",
    "    llm,\n",
    "    StrOutputParser()\n",
    ")\n",
    "\n",
    "# Perform a query and get a response\n",
    "query = \"example query text\"\n",
    "response = chain.invoke(query)\n",
    "\n",
    "# Print the response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the Chunk Size\n",
    "\n",
    "When using the `CharacterTextSplitter`, there are two parameters that are automatically set: `separator` and `is_separator_regex`. \n",
    "By default, the `separator` parameter is set to a double new line (`\\n\\n`). This means that if the text does not contain double new lines, it may not form a new chunk, even if it exceeds the specified chunk size.\n",
    "\n",
    "To ensure that chunks are formed around the correct chunk size, a custom separator can be defined. In this case, a single new line (`\\n`) can be used as the custom separator. This will help in forming chunks of the correct size and overlap.\n",
    "\n",
    "### How to modify the code to include a custom separator and add the question to the prompt?\n",
    "\n",
    "This code will ensure that the text is split into chunks of the correct size using a single new line as the separator. It also includes the question in the prompt to instruct the LLM to cite its sources, ensuring that the LLM provides the necessary citations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from langchain.document import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import os\n",
    "\n",
    "# Set the folder containing the text files\n",
    "folder_path = 'Big Star Collectibles'\n",
    "\n",
    "# Get a list of text files in the folder\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "# Initialize an empty list to hold chunked texts\n",
    "chunked_texts = []\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for file_name in file_list:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    # Open and read the entire file as a single string\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Create a CharacterTextSplitter object with a custom separator\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=128, chunk_overlap=32, separator='\\n')\n",
    "    \n",
    "    # Split the text into chunks\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    \n",
    "    # Loop through each chunk and create a Document object with metadata\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        metadata = {\n",
    "            'doc_title': file_name,\n",
    "            'chunk_number': i\n",
    "        }\n",
    "        doc = Document(page_content=chunk, metadata=metadata)\n",
    "        chunked_texts.append(doc)\n",
    "\n",
    "# Print the first chunk to verify\n",
    "print(chunked_texts[0].page_content)\n",
    "\n",
    "# Import necessary libraries for embedding and querying\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import RunnablePassthrough, StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Instantiate the HuggingFaceEmbeddings object\n",
    "embedding_function = HuggingFaceEmbeddings()\n",
    "\n",
    "# Create the vector store\n",
    "vector_store = FAISS.from_documents(chunked_texts, embedding_function)\n",
    "\n",
    "# Prepare the vector store for querying\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# Instantiate the OpenAI LLM\n",
    "llm = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Create a prompt template with source citation\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"Question: {question}\\nContext: {context}\\nPlease cite your sources.\"\n",
    ")\n",
    "\n",
    "# Create the chain\n",
    "chain = RunnablePassthrough(\n",
    "    retriever,\n",
    "    prompt_template,\n",
    "    llm,\n",
    "    StrOutputParser()\n",
    ")\n",
    "\n",
    "# Perform a query and get a response\n",
    "query = \"example query text\"\n",
    "response = chain.invoke(query)\n",
    "\n",
    "# Print the response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: Vector Embeddings for Images\n",
    "\n",
    "Images are one of the classic unstructured data types, and vector embeddings are crucial for comparing images. There are two main types of vectors used for this purpose: semantic vectors and visual vectors. These vectors describe images in fundamentally different ways.\n",
    "\n",
    "1. **Semantic Embeddings**:\n",
    "   - These embeddings describe the meaning of the image.\n",
    "   - Semantic embeddings are derived from deep learning models. In these models, image data passes from the input layer, through a series of hidden layers, to an output layer. The second-to-last layer captures all the meaning that the model has derived to make its prediction, classification, or segmentation. This layer is used as the semantic embedding.\n",
    "   - These embeddings are typically used in RAG applications because they capture the essence of what the image represents.\n",
    "\n",
    "2. **Visual or Pixel Embeddings**:\n",
    "   - These embeddings encode what the image literally looks like.\n",
    "   - Visual embeddings are also a long list of numbers. In image models trained in frameworks like PyTorch, images are compressed into a vector as the input to the model. These vectors capture the visual appearance of the image.\n",
    "   - While technically vector embeddings, visual embeddings are less commonly used in RAG applications compared to semantic embeddings.\n",
    "\n",
    "In summary, for RAG applications, the focus is primarily on semantic embeddings because they provide a meaningful representation of the image's content."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Vision Models 101\n",
    "\n",
    "To understand how machines compare images, it's essential to grasp how vision models work. Vision models are a type of deep neural network trained for computer vision tasks such as image classification, segmentation, and object detection.\n",
    "\n",
    "1. **History of Neural Networks**:\n",
    "   - **1960s**: The first neural networks were simple, modeled as layers of neurons fully connected to adjacent layers.\n",
    "   - **Progression**: Over time, different neural network architectures were developed to better handle various types of data.\n",
    "\n",
    "2. **Convolutional Neural Networks (CNNs)**:\n",
    "   - **Convolution**: A technique that helps in getting local context from images. It involves a filter that moves across the image, performing element-wise multiplication and summing the results to produce a new image.\n",
    "   - **Pooling**: Max pooling is a method to downsample the image by taking the maximum value in a region, which helps in reducing the dimensionality while retaining important features.\n",
    "   - **Architecture**: CNNs combine convolutional layers and pooling layers to extract and combine local contexts from different parts of the image.\n",
    "\n",
    "3. **Vision Transformers (ViTs)**:\n",
    "   - **Introduction**: Vision transformers apply the attention mechanism from transformer models (originally used for language) to computer vision.\n",
    "   - **Patches**: Images are divided into N by N patches, each turned into embeddings.\n",
    "   - **Processing**: These embeddings, combined with a class token and positional encoding, are fed into the transformer. The output is processed by a multilayer perceptron (MLP) to produce logits for tasks like object detection or segmentation.\n",
    "\n",
    "Here's a visual representation of how a convolution works:\n",
    "\n",
    "- **Convolution**: Imagine a 2D image filled with numbers. A 3x3 filter moves across the image, performing element-wise multiplication with the image values and summing the results to produce a new image.\n",
    "- **Pooling**: In max pooling, the maximum value in a region (e.g., 12, 20, 8, 12) is taken to represent that region (e.g., 20).\n",
    "\n",
    "Vision transformers operate similarly but use patches and attention mechanisms to process the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Getting Semantic Vectors\n",
    "\n",
    "In this section, a semantic vector from an image using the OpenCLIP embeddings with LangChain will obtain. \n",
    "\n",
    "1. **Import Libraries**:\n",
    "   - `OpenCLIPEmbeddings` from LangChain is used to load the embedding model.\n",
    "   - `glob` is used to retrieve all image file paths in the specified directory.\n",
    "\n",
    "2. **Get Image File Paths**:\n",
    "   - `glob.glob` retrieves all `.jpg` files in the specified folder.\n",
    "\n",
    "3. **Load Embedding Model**:\n",
    "   - `OpenCLIPEmbeddings` is instantiated to create the embedding model.\n",
    "\n",
    "4. **Embed Images**:\n",
    "   - `embed_image` function is called with the list of image file paths to generate embeddings.\n",
    "\n",
    "5. **Examine Embeddings**:\n",
    "   - The first embedding is printed to see what it looks like.\n",
    "   - The length of the first embedding is printed to check its dimensionality, which should be 1024.\n",
    "\n",
    "This process will generate semantic vectors for the images, which can then be used for various tasks such as image comparison or retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from langchain.embeddings import OpenCLIPEmbeddings\n",
    "import glob\n",
    "\n",
    "# Get all image file paths\n",
    "image_folder_path = 'path/to/your/images'\n",
    "image_file_paths = glob.glob(f\"{image_folder_path}/*.jpg\")\n",
    "\n",
    "# Load the OpenCLIP embedding model\n",
    "embedding_model = OpenCLIPEmbeddings()\n",
    "\n",
    "# Embed the images\n",
    "embeddings = embedding_model.embed_image(image_file_paths)\n",
    "\n",
    "# Examine the first embedding\n",
    "print(embeddings[0])\n",
    "\n",
    "# Check the dimensionality of the embeddings\n",
    "print(len(embeddings[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Storing Image Vectors\n",
    "\n",
    "This section covers how to store image vectors, similar to the process used in the text RAG chapter. Here are the steps:\n",
    "\n",
    "1. **Imports**:\n",
    "   - **LangChain Imports**: `Document`, `FAISS`, and `OpenCLIPEmbeddings`.\n",
    "   - **Other Imports**: `glob` for handling multiple file paths, and `base64` for converting images into base64 strings for the LLM.\n",
    "\n",
    "2. **Create an Empty List**:\n",
    "   - Initialize an empty list to hold the documents.\n",
    "\n",
    "3. **Define the Encode Image Function**:\n",
    "   - This function takes a file path, opens the file, reads it as bytes, and returns a UTF-encoded string of the file.\n",
    "\n",
    "4. **Loop Through Image Paths**:\n",
    "   - For each image path, create a document containing the encoded image and metadata (including the image path).\n",
    "\n",
    "5. **Store in FAISS Vector Database**:\n",
    "   - Use the `OpenCLIPEmbeddings` as the embedding function to store the documents in a FAISS vector database.\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "1. **Imports**:\n",
    "   - `Document`, `FAISS`, and `OpenCLIPEmbeddings` from LangChain.\n",
    "   - `glob` for retrieving image file paths.\n",
    "   - `base64` for encoding images.\n",
    "\n",
    "2. **Get Image File Paths**:\n",
    "   - `glob.glob` retrieves all `.jpg` files in the specified folder.\n",
    "\n",
    "3. **Initialize List**:\n",
    "   - An empty list `documents` is created to hold the encoded images and their metadata.\n",
    "\n",
    "4. **Encode Image Function**:\n",
    "   - `encode_image` function reads the image file as bytes and encodes it to a base64 string.\n",
    "\n",
    "5. **Create Documents**:\n",
    "   - Loop through each image path, encode the image, and create a `Document` object with the encoded image and metadata.\n",
    "\n",
    "6. **Create Vector Store**:\n",
    "   - Use `OpenCLIPEmbeddings` to create the FAISS vector store from the documents.\n",
    "\n",
    "This code will store the image vectors in a FAISS vector database, making them ready for retrieval and comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from langchain.document import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenCLIPEmbeddings\n",
    "import glob\n",
    "import base64\n",
    "\n",
    "# Get all image file paths\n",
    "image_folder_path = 'path/to/your/images'\n",
    "image_file_paths = glob.glob(f\"{image_folder_path}/*.jpg\")\n",
    "\n",
    "# Initialize an empty list to hold documents\n",
    "documents = []\n",
    "\n",
    "# Define a function to encode images\n",
    "def encode_image(file_path):\n",
    "    with open(file_path, 'rb') as image_file:\n",
    "        image_bytes = image_file.read()\n",
    "    return base64.b64encode(image_bytes).decode('utf-8')\n",
    "\n",
    "# Loop through each image path and create a document\n",
    "for file_path in image_file_paths:\n",
    "    encoded_image = encode_image(file_path)\n",
    "    metadata = {'image_path': file_path}\n",
    "    doc = Document(page_content=encoded_image, metadata=metadata)\n",
    "    documents.append(doc)\n",
    "\n",
    "# Load the OpenCLIP embedding model\n",
    "embedding_model = OpenCLIPEmbeddings()\n",
    "\n",
    "# Create the FAISS vector store\n",
    "vector_store = FAISS.from_documents(documents, embedding_model)\n",
    "\n",
    "# Print the first document to verify\n",
    "print(documents[0].page_content)\n",
    "print(documents[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Comparing Images Semantically\n",
    "\n",
    "Now that the data is stored in a vector database, the next step is to compare images to find the most similar sets. Here's how to do it:\n",
    "\n",
    "1. **Turn Vector Store into a Retriever**:\n",
    "   - Convert the vector store into a retriever to facilitate querying.\n",
    "\n",
    "2. **Retrieve Images**:\n",
    "   - Use the retriever to get images by passing the encoded string of an image.\n",
    "   - For example, pass the encoded string of the first cat image to retrieve the top four most similar images.\n",
    "\n",
    "Here's the code to compare images semantically:\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "1. **Imports**:\n",
    "   - `Document`, `FAISS`, and `OpenCLIPEmbeddings` from LangChain.\n",
    "   - `glob` for retrieving image file paths.\n",
    "   - `base64` for encoding images.\n",
    "\n",
    "2. **Get Image File Paths**:\n",
    "   - `glob.glob` retrieves all `.jpg` files in the specified folder.\n",
    "\n",
    "3. **Initialize List**:\n",
    "   - An empty list `documents` is created to hold the encoded images and their metadata.\n",
    "\n",
    "4. **Encode Image Function**:\n",
    "   - `encode_image` function reads the image file as bytes and encodes it to a base64 string.\n",
    "\n",
    "5. **Create Documents**:\n",
    "   - Loop through each image path, encode the image, and create a `Document` object with the encoded image and metadata.\n",
    "\n",
    "6. **Create Vector Store**:\n",
    "   - Use `OpenCLIPEmbeddings` to create the FAISS vector store from the documents.\n",
    "\n",
    "7. **Prepare Retriever**:\n",
    "   - Convert the vector store into a retriever.\n",
    "\n",
    "8. **Query for Similar Images**:\n",
    "   - Encode the first image to use as a query.\n",
    "   - Use the retriever to find the top four most similar images.\n",
    "\n",
    "This code will compare the images semantically and retrieve the most similar ones based on the encoded query image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenCLIPEmbeddings\n",
    "from langchain.document import Document\n",
    "import glob\n",
    "import base64\n",
    "\n",
    "# Get all image file paths\n",
    "image_folder_path = 'path/to/your/images'\n",
    "image_file_paths = glob.glob(f\"{image_folder_path}/*.jpg\")\n",
    "\n",
    "# Initialize an empty list to hold documents\n",
    "documents = []\n",
    "\n",
    "# Define a function to encode images\n",
    "def encode_image(file_path):\n",
    "    with open(file_path, 'rb') as image_file:\n",
    "        image_bytes = image_file.read()\n",
    "    return base64.b64encode(image_bytes).decode('utf-8')\n",
    "\n",
    "# Loop through each image path and create a document\n",
    "for file_path in image_file_paths:\n",
    "    encoded_image = encode_image(file_path)\n",
    "    metadata = {'image_path': file_path}\n",
    "    doc = Document(page_content=encoded_image, metadata=metadata)\n",
    "    documents.append(doc)\n",
    "\n",
    "# Load the OpenCLIP embedding model\n",
    "embedding_model = OpenCLIPEmbeddings()\n",
    "\n",
    "# Create the FAISS vector store\n",
    "vector_store = FAISS.from_documents(documents, embedding_model)\n",
    "\n",
    "# Prepare the vector store for querying\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# Encode the first image to use as a query\n",
    "query_image_path = image_file_paths[0]\n",
    "query_image_encoded = encode_image(query_image_path)\n",
    "\n",
    "# Perform a query to find similar images\n",
    "results = retriever.invoke(query_image_encoded)\n",
    "\n",
    "# Print the top 4 results\n",
    "for result in results:\n",
    "    print(result.page_content)\n",
    "    print(result.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge: Find the Dog Most Similar to a Cat\n",
    "\n",
    "To find the dog that looks most similar to the cats using semantic embeddings, follow these steps:\n",
    "\n",
    "1. **Prepare the Environment**:\n",
    "   - Ensure all vectors are stored in the vector store.\n",
    "   - Have the retriever ready to query the vector store.\n",
    "\n",
    "2. **Get Dog Image Paths**:\n",
    "   - Create a list of paths for the dog images.\n",
    "\n",
    "3. **Create a Dictionary for Mapping**:\n",
    "   - Map the paths of the dogs to the paths of the cats in an inversely-weighted order.\n",
    "\n",
    "4. **Retrieve Top Images for Each Dog**:\n",
    "   - For each dog image, retrieve the top four most similar images based on the Base64 encoding of the dog image.\n",
    "   - Initialize a counter (`cats_retrieved`) to zero.\n",
    "\n",
    "5. **Calculate Cat Scores**:\n",
    "   - Loop through the enumerated list of returned documents.\n",
    "   - Check if the word \"cat\" is in the source returned.\n",
    "   - Add `4 - i` (inverse weight based on rank) to `cats_retrieved` if the image is a cat.\n",
    "\n",
    "6. **Determine the Most Similar Dog**:\n",
    "   - Attach the cat score to each dog image.\n",
    "   - Identify the dog with the highest cat score.\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "1. **Imports**:\n",
    "   - `Document`, `FAISS`, and `OpenCLIPEmbeddings` from LangChain.\n",
    "   - `glob` for retrieving image file paths.\n",
    "   - `base64` for encoding images.\n",
    "\n",
    "2. **Get Image File Paths**:\n",
    "   - `glob.glob` retrieves all `.jpg` files in the specified folder.\n",
    "   - Separate paths for dog and cat images.\n",
    "\n",
    "3. **Initialize List**:\n",
    "   - An empty list `documents` is created to hold the encoded images and their metadata.\n",
    "\n",
    "4. **Encode Image Function**:\n",
    "   - `encode_image` function reads the image file as bytes and encodes it to a base64 string.\n",
    "\n",
    "5. **Create Documents**:\n",
    "   - Loop through each image path, encode the image, and create a `Document` object with the encoded image and metadata.\n",
    "\n",
    "6. **Create Vector Store**:\n",
    "   - Use `OpenCLIPEmbeddings` to create the FAISS vector store from the documents.\n",
    "\n",
    "7. **Prepare Retriever**:\n",
    "   - Convert the vector store into a retriever.\n",
    "\n",
    "8. **Find Most Similar Dog**:\n",
    "   - For each dog image, retrieve the top four most similar images.\n",
    "   - Calculate the cat score based on the presence of cat images in the results.\n",
    "   - Identify the dog with the highest cat score.\n",
    "\n",
    "This code will help you find the dog that looks most similar to the cats based on the semantic embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenCLIPEmbeddings\n",
    "from langchain.document import Document\n",
    "import glob\n",
    "import base64\n",
    "\n",
    "# Get all image file paths\n",
    "image_folder_path = 'path/to/your/images'\n",
    "image_file_paths = glob.glob(f\"{image_folder_path}/*.jpg\")\n",
    "\n",
    "# Separate dog and cat image paths\n",
    "dog_image_paths = [path for path in image_file_paths if 'dog' in path]\n",
    "cat_image_paths = [path for path in image_file_paths if 'cat' in path]\n",
    "\n",
    "# Initialize an empty list to hold documents\n",
    "documents = []\n",
    "\n",
    "# Define a function to encode images\n",
    "def encode_image(file_path):\n",
    "    with open(file_path, 'rb') as image_file:\n",
    "        image_bytes = image_file.read()\n",
    "    return base64.b64encode(image_bytes).decode('utf-8')\n",
    "\n",
    "# Loop through each image path and create a document\n",
    "for file_path in image_file_paths:\n",
    "    encoded_image = encode_image(file_path)\n",
    "    metadata = {'image_path': file_path}\n",
    "    doc = Document(page_content=encoded_image, metadata=metadata)\n",
    "    documents.append(doc)\n",
    "\n",
    "# Load the OpenCLIP embedding model\n",
    "embedding_model = OpenCLIPEmbeddings()\n",
    "\n",
    "# Create the FAISS vector store\n",
    "vector_store = FAISS.from_documents(documents, embedding_model)\n",
    "\n",
    "# Prepare the vector store for querying\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# Function to find the dog most similar to cats\n",
    "def find_most_similar_dog(dog_paths, retriever):\n",
    "    cat_scores = {}\n",
    "    for dog_path in dog_paths:\n",
    "        encoded_dog_image = encode_image(dog_path)\n",
    "        results = retriever.invoke(encoded_dog_image)\n",
    "        cats_retrieved = 0\n",
    "        for i, result in enumerate(results):\n",
    "            if 'cat' in result.metadata['image_path']:\n",
    "                cats_retrieved += (4 - i)\n",
    "        cat_scores[dog_path] = cats_retrieved\n",
    "    most_similar_dog = max(cat_scores, key=cat_scores.get)\n",
    "    return most_similar_dog, cat_scores[most_similar_dog]\n",
    "\n",
    "# Find the dog most similar to cats\n",
    "most_similar_dog, score = find_most_similar_dog(dog_image_paths, retriever)\n",
    "\n",
    "# Print the result\n",
    "print(f\"The dog most similar to cats is: {most_similar_dog} with a score of {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: Multimodality\n",
    "\n",
    "### What is Multimodality?\n",
    "- **Definition**: Multimodal AI applications deal with multiple types of data.\n",
    "- **Origin**: The term comes from \"multi\" (many) and \"modal\" (types).\n",
    "- **Human-like Power**: Multimodal AI mimics human senses (sight, hearing, taste, touch, smell).\n",
    "- **Common Modalities in AI**: \n",
    "  - Images and text\n",
    "  - Images and audio\n",
    "  - Video\n",
    "- **Other Examples**:\n",
    "  - PDFs and CSVs\n",
    "  - Texts and tables\n",
    "  - Tables and graphs\n",
    "\n",
    "- **Explanation**: \n",
    "  - The core idea behind multimodal AI applications is that they deal with multiple types of data.\n",
    "  - Multimodal AI is popular because it gives AI more human-like power.\n",
    "  - Humans have a multimodal interface with the world through senses like sight, hearing, taste, touch, and smell.\n",
    "  - In AI, the most emulated modalities are sight and hearing.\n",
    "  - While the term \"multimodal\" is debated, examples include images and text, images and audio, and video.\n",
    "  - These correspond to human senses like sight and sound.\n",
    "  - Machines process images and texts as different kinds of data, even though both relate to sight.\n",
    "  - Some multimodal data examples are easier to relate to human senses, while others are more contested.\n",
    "  - Examples include PDFs and CSVs, texts and tables, and tables and graphs.\n",
    "  - These examples fall into the sight category but need different machine learning models for processing.\n",
    "\n",
    "### 3.1. Ways to Do Multimodal RAG (Retrieval-Augmented Generation)\n",
    "- **Two Main Methods**:\n",
    "  1. **Single Multimodal Model**: Handles multiple types of data.\n",
    "  2. **Multiple Models**: One for each data type.\n",
    "- **Single Multimodal Model Advantages**:\n",
    "  - One vector store\n",
    "  - One embedding model\n",
    "  - Consistent dimensionality for embeddings\n",
    "- **Multiple Models Approach**:\n",
    "  - Separate models and vector stores for each data type\n",
    "  - Manual process for routing data\n",
    "- **Combination Method**:\n",
    "  - Multiple multimodal models\n",
    "  - Used for diverse data types or re-ranking vector results\n",
    "\n",
    "- **Explanation**:\n",
    "  - The basic idea behind multimodal RAG is the same as multimodal data.\n",
    "  - Two main ways to do multimodal RAG:\n",
    "    1. Use one multimodal model that can handle multiple types of data.\n",
    "    2. Use multiple models, usually one for each type of data.\n",
    "  - **Single Multimodal Model**:\n",
    "    - Uses one multimodal embedding model to process multiple types of data, usually images and text.\n",
    "    - Advantages: One vector store, one embedding model, consistent dimensionality for embeddings.\n",
    "    - Example: CLIP model with image and text options.\n",
    "    - Frameworks like LangChain or LlamaIndex can handle this for you.\n",
    "  - **Multiple Models**:\n",
    "    - Uses multiple embedding models and multiple search modes for different types of data.\n",
    "    - Requires routing each type of data to the right model for storing and searching.\n",
    "    - Separate vector stores for each model.\n",
    "    - Vectors of the same size can be compared, but different models generate embeddings of different lengths.\n",
    "    - Manual process for routing data.\n",
    "  - **Combination Method**:\n",
    "    - Uses multiple multimodal models.\n",
    "    - Rarely used in practice.\n",
    "    - Reasons to use: Handling many different types of data or re-ranking vector results.\n",
    "    - Routes data to different models and vector stores based on data type.\n",
    "    - Uses tagging system for re-ranking.\n",
    "\n",
    "### 3.2. Introduction to Multimodal Embedding Models\n",
    "- **Definition**: Models that can embed multiple types of data.\n",
    "- **Training**: Often trained on pairs of data (e.g., images and text).\n",
    "- **Examples**:\n",
    "  - **CLIP (Contrastive Language-Image Pretraining)**:\n",
    "    - Two encoders: one for images, one for text\n",
    "    - Trained on 400 million image-text pairs\n",
    "  - **GPT-4o**: A large language model that has evolved to become multimodal.\n",
    "  - **LLaVa**: Combines an image encoder with a large language model (LLM).\n",
    "\n",
    "- **Explanation**:\n",
    "  - Multimodal embedding models can embed multiple types of data.\n",
    "  - Different functions internally to embed each type of data.\n",
    "  - Embedding each type of data is a different process.\n",
    "  - Often requires pre-processing.\n",
    "  - Different parts of the model are trained on and for different types of data.\n",
    "  - Common practice: Train on pairs of data (e.g., images and text).\n",
    "  - **CLIP**:\n",
    "    - OpenAI model with two encoders (images and text).\n",
    "    - Trained on 400 million pairs of image and text data.\n",
    "    - Contrastive learning aligns two modalities.\n",
    "    - Encoders represent pairs closely and unpaired combos far apart.\n",
    "    - Same embedding space and dimensionality for vectors.\n",
    "  - **GPT-4o**: Multimodal large language model from OpenAI.\n",
    "  - **LLaVa**: Combines image encoder with Vicuna, a large language model.\n",
    "\n",
    "### 3.3. Embedding and Storing Data\n",
    "- **Review**:\n",
    "  - Using LangChain to get OpenCLIPEmbeddings.\n",
    "  - Storing vectors into FAISS.\n",
    "- **Process**:\n",
    "  - Grab images and encode them into Base64 for the LLM.\n",
    "  - Create documents from images.\n",
    "  - Use OpenCLIPEmbeddings with documents.\n",
    "  - Store embeddings into the FAISS vector database.\n",
    "\n",
    "### 3.4. Query Images with Text\n",
    "- **Process**:\n",
    "  - Create a retriever object from the vector store.\n",
    "  - Import `BytesIO` and `Images` for handling byte and image data.\n",
    "  - Create three functions:\n",
    "    1. **Resizing Function**: Takes a Base64 string, resizes the image, and returns it as a Base64-encoded string.\n",
    "    2. **Base64 Check Function**: Checks if a string is in Base64 format.\n",
    "    3. **Split Function**: Splits image and text input.\n",
    "  - Create a prompt for the multimodal RAG app using imports like `HumanMessage`, `RunnableLambda`, and `ChatOpenAI`.\n",
    "  - Make a function to split image and text data and format them into prompts.\n",
    "  - Use the foundational model (GPT-4o mini) and create a chain.\n",
    "  - Invoke the chain to query images with text (e.g., looking for a rottweiler in images).\n",
    "\n",
    "### Explanation of the following code:\n",
    "1. **Imports**: Corrected the import statements to use `langchain` instead of `langchain_core`.\n",
    "2. **Encoding Function**: Encodes images to Base64 strings.\n",
    "3. **Document Creation**: Creates documents with encoded images and metadata.\n",
    "4. **Vector Store**: Creates a vector store from the documents using `OpenCLIPEmbeddings`.\n",
    "5. **Retriever**: Initializes a retriever from the vector store.\n",
    "6. **Resizing Function**: Resizes Base64 images to avoid server errors.\n",
    "7. **Base64 Check Function**: Checks if a string is Base64 encoded.\n",
    "8. **Split Function**: Splits image and text types, resizing images if necessary.\n",
    "9. **Prompt Creation**: Creates prompts for the multimodal RAG app.\n",
    "10. **Chain Initialization**: Initializes the foundational model and creates a chain using both context and question.\n",
    "11. **Chain Invocation**: Uses `chain.invoke` to query images with text.\n",
    "12. **Example Usage**: Queries for a rottweiler in images with context and question, and retrieves documents based on various text queries and encoded images, printing the metadata of the results.\n",
    "\n",
    "This should resolve the import issue and ensure the code runs correctly. If you have any more questions or need further adjustments, feel free to ask!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_experimental.open_clip import OpenCLIPEmbeddings\n",
    "import glob\n",
    "import base64\n",
    "\n",
    "# Function to encode image to Base64 string\n",
    "def encode_image(path):\n",
    "    with open(path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "# Get all image paths\n",
    "paths = glob.glob('../images/*.jpeg', recursive=True)\n",
    "\n",
    "# Create a list of documents with encoded images\n",
    "lc_docs = []\n",
    "for path in paths:\n",
    "    doc = Document(\n",
    "        page_content=encode_image(path),\n",
    "        metadata={\n",
    "            'source': path\n",
    "        }\n",
    "    )\n",
    "    lc_docs.append(doc)\n",
    "\n",
    "# Create a vector store from documents using OpenCLIPEmbeddings\n",
    "vector_store = FAISS.from_documents(lc_docs, embedding=OpenCLIPEmbeddings())\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "# Function to resize Base64 image\n",
    "def resize_base64_image(base64_string, size=(128, 128)):\n",
    "    # Decode the Base64 string\n",
    "    img_data = base64.b64decode(base64_string)\n",
    "    img = Image.open(BytesIO(img_data))\n",
    "\n",
    "    # Resize the image\n",
    "    resized_img = img.resize(size, Image.LANCZOS)\n",
    "\n",
    "    # Save the resized image to a bytes buffer\n",
    "    buffered = BytesIO()\n",
    "    resized_img.save(buffered, format=img.format)\n",
    "\n",
    "    # Encode the resized image to Base64\n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "# Function to check if a string is Base64 encoded\n",
    "def is_base64(s):\n",
    "    try:\n",
    "        return base64.b64encode(base64.b64decode(s)) == s.encode()\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# Function to split image and text types, resizing images if necessary\n",
    "def split_image_text_types(docs):\n",
    "    images = []\n",
    "    text = []\n",
    "    for doc in docs:\n",
    "        doc_content = doc.page_content  # Extract Document contents\n",
    "        if is_base64(doc_content):\n",
    "            # Resize image to avoid OAI server error\n",
    "            images.append(resize_base64_image(doc_content))  # base64 encoded str\n",
    "        else:\n",
    "            text.append(doc_content)\n",
    "    return {\"images\": images, \"texts\": text}\n",
    "\n",
    "from langchain.prompts import HumanMessage\n",
    "from langchain.output_parsers import StrOutputParser\n",
    "from langchain.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Function to create prompt for the multimodal RAG app\n",
    "def prompt_func(data_dict):\n",
    "    # Joining the context texts into a single string\n",
    "    formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
    "    messages = []\n",
    "\n",
    "    # Adding image(s) to the messages if present\n",
    "    if data_dict[\"context\"][\"images\"]:\n",
    "        image_message = {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": f\"data:image/jpeg;base64,{data_dict['context']['images'][0]}\"\n",
    "            },\n",
    "        }\n",
    "        messages.append(image_message)\n",
    "\n",
    "    # Adding the text message for analysis\n",
    "    text_message = {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            \"As an animal lover, your task is to analyze and interpret images of cute animals. \"\n",
    "            \"Please use your extensive knowledge and analytical skills to provide a \"\n",
    "            \"summary that includes:\\n\"\n",
    "            \"- A detailed description of the visual elements in the image.\\n\"\n",
    "            f\"User-provided keywords: {data_dict['question']}\\n\\n\"\n",
    "            \"Text and/or tables:\\n\"\n",
    "            f\"{formatted_texts}\"\n",
    "        ),\n",
    "    }\n",
    "    messages.append(text_message)\n",
    "\n",
    "    return [HumanMessage(content=messages)]\n",
    "\n",
    "foundation = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\", max_tokens=1024)\n",
    "\n",
    "# RAG pipeline\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever | RunnableLambda(split_image_text_types),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | RunnableLambda(prompt_func)\n",
    "    | foundation\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Example usage: Querying for a rottweiler in images with context and question\n",
    "result = chain.invoke({\n",
    "    \"docs\": [\"<Base64_encoded_image_string>\", \"rottweiler\"],\n",
    "    \"context\": {\"texts\": [\"This is a search for dog breeds.\"], \"images\": []},\n",
    "    \"question\": \"Find images of a rottweiler\"\n",
    "})\n",
    "\n",
    "# Output the result\n",
    "print(result)\n",
    "\n",
    "# Retrieve documents based on text query \"rottweiler\"\n",
    "docs = retriever.invoke(\"rottweiler\", k=4)  # dog 5\n",
    "for doc in docs:\n",
    "    print(doc.metadata)\n",
    "\n",
    "# Retrieve documents based on encoded image of cat_1.jpeg\n",
    "docs = retriever.invoke(encode_image(\"../images/cat_1.jpeg\"), k=4)  # cat 1\n",
    "for doc in docs:\n",
    "    print(doc.metadata)\n",
    "\n",
    "# Retrieve documents based on text query \"gray cat with long hair in a field\"\n",
    "docs = retriever.invoke(\"gray cat with long hair in a field\", k=4)  # cat 2\n",
    "for doc in docs:\n",
    "    print(doc.metadata)\n",
    "\n",
    "# Retrieve documents based on text query \"golden retriever playing with orange ball\"\n",
    "docs = retriever.invoke(\"golden retriever playing with orange ball\", k=4)  # dog 2\n",
    "for doc in docs:\n",
    "    print(doc.metadata)\n",
    "\n",
    "# Retrieve documents based on text query \"golden retriever in field with a sunny blurred background\"\n",
    "docs = retriever.invoke(\"golden retriever in field with a sunny blurred background\", k=4)  # dog 4\n",
    "for doc in docs:\n",
    "    print(doc.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Anomalies in Embeddings\n",
    "To find anomalies in your embeddings, you can follow these steps to analyze the data stored in your vector database and identify mismatches between text and images. Here's a code example to help you get started:\n",
    "### Explanation:\n",
    "1. **Imports**: Corrected the import statements to use `langchain` instead of `langchain_core`.\n",
    "2. **Encoding Function**: Encodes images to Base64 strings.\n",
    "3. **Document Creation**: Creates documents with encoded images and metadata.\n",
    "4. **Vector Store**: Creates a vector store from the documents using `OpenCLIPEmbeddings`.\n",
    "5. **Retriever**: Initializes a retriever from the vector store.\n",
    "6. **Resizing Function**: Resizes Base64 images to avoid server errors.\n",
    "7. **Base64 Check Function**: Checks if a string is Base64 encoded.\n",
    "8. **Split Function**: Splits image and text types, resizing images if necessary.\n",
    "9. **Prompt Creation**: Creates prompts for the multimodal RAG app.\n",
    "10. **Chain Initialization**: Initializes the foundational model and creates a chain using both context and question.\n",
    "11. **Chain Invocation**: Uses `chain.invoke` to query images with text.\n",
    "12. **Example Usage**: Queries for a rottweiler in images with context and question, and retrieves documents based on various text queries and encoded images, printing the metadata of the results.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_experimental.open_clip import OpenCLIPEmbeddings\n",
    "import glob\n",
    "import base64\n",
    "\n",
    "# Function to encode image to Base64 string\n",
    "def encode_image(path):\n",
    "    with open(path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "# Get all image paths\n",
    "paths = glob.glob('../images/*.jpeg', recursive=True)\n",
    "\n",
    "# Create a list of documents with encoded images\n",
    "lc_docs = []\n",
    "for path in paths:\n",
    "    doc = Document(\n",
    "        page_content=encode_image(path),\n",
    "        metadata={\n",
    "            'source': path\n",
    "        }\n",
    "    )\n",
    "    lc_docs.append(doc)\n",
    "\n",
    "# Create a vector store from documents using OpenCLIPEmbeddings\n",
    "vector_store = FAISS.from_documents(lc_docs, embedding=OpenCLIPEmbeddings())\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "# Function to resize Base64 image\n",
    "def resize_base64_image(base64_string, size=(128, 128)):\n",
    "    # Decode the Base64 string\n",
    "    img_data = base64.b64decode(base64_string)\n",
    "    img = Image.open(BytesIO(img_data))\n",
    "\n",
    "    # Resize the image\n",
    "    resized_img = img.resize(size, Image.LANCZOS)\n",
    "\n",
    "    # Save the resized image to a bytes buffer\n",
    "    buffered = BytesIO()\n",
    "    resized_img.save(buffered, format=img.format)\n",
    "\n",
    "    # Encode the resized image to Base64\n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "# Function to check if a string is Base64 encoded\n",
    "def is_base64(s):\n",
    "    try:\n",
    "        return base64.b64encode(base64.b64decode(s)) == s.encode()\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# Function to split image and text types, resizing images if necessary\n",
    "def split_image_text_types(docs):\n",
    "    images = []\n",
    "    text = []\n",
    "    for doc in docs:\n",
    "        doc_content = doc.page_content  # Extract Document contents\n",
    "        if is_base64(doc_content):\n",
    "            # Resize image to avoid OAI server error\n",
    "            images.append(resize_base64_image(doc_content))  # base64 encoded str\n",
    "        else:\n",
    "            text.append(doc_content)\n",
    "    return {\"images\": images, \"texts\": text}\n",
    "\n",
    "from langchain.prompts import HumanMessage\n",
    "from langchain.output_parsers import StrOutputParser\n",
    "from langchain.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Function to create prompt for the multimodal RAG app\n",
    "def prompt_func(data_dict):\n",
    "    # Joining the context texts into a single string\n",
    "    formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
    "    messages = []\n",
    "\n",
    "    # Adding image(s) to the messages if present\n",
    "    if data_dict[\"context\"][\"images\"]:\n",
    "        image_message = {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": f\"data:image/jpeg;base64,{data_dict['context']['images'][0]}\"\n",
    "            },\n",
    "        }\n",
    "        messages.append(image_message)\n",
    "\n",
    "    # Adding the text message for analysis\n",
    "    text_message = {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            \"As an animal lover, your task is to analyze and interpret images of cute animals. \"\n",
    "            \"Please use your extensive knowledge and analytical skills to provide a \"\n",
    "            \"summary that includes:\\n\"\n",
    "            \"- A detailed description of the visual elements in the image.\\n\"\n",
    "            f\"User-provided keywords: {data_dict['question']}\\n\\n\"\n",
    "            \"Text and/or tables:\\n\"\n",
    "            f\"{formatted_texts}\"\n",
    "        ),\n",
    "    }\n",
    "    messages.append(text_message)\n",
    "\n",
    "    return [HumanMessage(content=messages)]\n",
    "\n",
    "foundation = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\", max_tokens=1024)\n",
    "\n",
    "# RAG pipeline\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever | RunnableLambda(split_image_text_types),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | RunnableLambda(prompt_func)\n",
    "    | foundation\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Example usage: Querying for a rottweiler in images with context and question\n",
    "result = chain.invoke({\n",
    "    \"docs\": [\"<Base64_encoded_image_string>\", \"rottweiler\"],\n",
    "    \"context\": {\"texts\": [\"This is a search for dog breeds.\"], \"images\": []},\n",
    "    \"question\": \"Find images of a rottweiler\"\n",
    "})\n",
    "\n",
    "# Output the result\n",
    "print(result)\n",
    "\n",
    "# Retrieve documents based on text query \"rottweiler\"\n",
    "docs = retriever.invoke(\"rottweiler\", k=4)  # dog 5\n",
    "for doc in docs:\n",
    "    print(doc.metadata)\n",
    "\n",
    "# Retrieve documents based on encoded image of cat_1.jpeg\n",
    "docs = retriever.invoke(encode_image(\"../images/cat_1.jpeg\"), k=4)  # cat 1\n",
    "for doc in docs:\n",
    "    print(doc.metadata)\n",
    "\n",
    "# Retrieve documents based on text query \"gray cat with long hair in a field\"\n",
    "docs = retriever.invoke(\"gray cat with long hair in a field\", k=4)  # cat 2\n",
    "for doc in docs:\n",
    "    print(doc.metadata)\n",
    "\n",
    "# Retrieve documents based on text query \"golden retriever playing with orange ball\"\n",
    "docs = retriever.invoke(\"golden retriever playing with orange ball\", k=4)  # dog 2\n",
    "for doc in docs:\n",
    "    print(doc.metadata)\n",
    "\n",
    "# Retrieve documents based on text query \"golden retriever in field with a sunny blurred background\"\n",
    "docs = retriever.invoke(\"golden retriever in field with a sunny blurred background\", k=4)  # dog 4\n",
    "for doc in docs:\n",
    "    print(doc.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "1. **Imports**: Corrected the import statements to use `langchain` instead of `langchain_core`.\n",
    "2. **Encoding Function**: Encodes images to Base64 strings.\n",
    "3. **Document Creation**: Creates documents with encoded images and metadata.\n",
    "4. **Vector Store**: Creates a vector store from the documents using `OpenCLIPEmbeddings`.\n",
    "5. **Retriever**: Initializes a retriever from the vector store.\n",
    "6. **Queries**: Retrieves documents based on various text queries and encoded images, printing the metadata of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_experimental.open_clip import OpenCLIPEmbeddings\n",
    "import glob\n",
    "import base64\n",
    "\n",
    "# Function to encode image to Base64 string\n",
    "def encode_image(path):\n",
    "    with open(path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "# Get all image paths\n",
    "paths = glob.glob('../images/*.jpeg', recursive=True)\n",
    "\n",
    "# Create a list of documents with encoded images\n",
    "lc_docs = []\n",
    "for path in paths:\n",
    "    doc = Document(\n",
    "        page_content=encode_image(path),\n",
    "        metadata={\n",
    "            'source': path\n",
    "        }\n",
    "    )\n",
    "    lc_docs.append(doc)\n",
    "\n",
    "# Create a vector store from documents using OpenCLIPEmbeddings\n",
    "vector_store = FAISS.from_documents(lc_docs, embedding=OpenCLIPEmbeddings())\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# Retrieve documents based on text query \"rottweiler\"\n",
    "docs = retriever.invoke(\"rottweiler\", k=4)  # dog 5\n",
    "for doc in docs:\n",
    "    print(doc.metadata)\n",
    "\n",
    "# Retrieve documents based on encoded image of cat_1.jpeg\n",
    "docs = retriever.invoke(encode_image(\"../images/cat_1.jpeg\"), k=4)  # cat 1\n",
    "for doc in docs:\n",
    "    print(doc.metadata)\n",
    "\n",
    "# Retrieve documents based on text query \"gray cat with long hair in a field\"\n",
    "docs = retriever.invoke(\"gray cat with long hair in a field\", k=4)  # cat 2\n",
    "for doc in docs:\n",
    "    print(doc.metadata)\n",
    "\n",
    "# Retrieve documents based on text query \"golden retriever playing with orange ball\"\n",
    "docs = retriever.invoke(\"golden retriever playing with orange ball\", k=4)  # dog 2\n",
    "for doc in docs:\n",
    "    print(doc.metadata)\n",
    "\n",
    "# Retrieve documents based on text query \"golden retriever in field with a sunny blurred background\"\n",
    "docs = retriever.invoke(\"golden retriever in field with a sunny blurred background\", k=4)  # dog 4\n",
    "for doc in docs:\n",
    "    print(doc.metadata)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
