{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG Applications with Vector Databases\n",
    "Retrieval-Augmented Generation (RAG) indeed leverages vector databases to enhance the capabilities of language models by integrating external knowledge. This approach is particularly useful for applications requiring up-to-date information and domain-specific knowledge.\n",
    "\n",
    "Here are some key aspects of state-of-the-art RAG methods:\n",
    "\n",
    "1. **Chunking**: This involves breaking down large documents into smaller, manageable pieces or \"chunks\" that can be individually indexed and retrieved. This improves the efficiency and accuracy of the retrieval process[1](https://www.promptingguide.ai/research/rag).\n",
    "\n",
    "2. **Embedding**: Embeddings are numerical representations of text that capture semantic meaning. By embedding both the query and the chunks, the system can effectively compare and retrieve the most relevant information[1](https://www.promptingguide.ai/research/rag).\n",
    "\n",
    "3. **Metadata Usage**: Incorporating metadata (such as document type, date, and author) can enhance the retrieval process by providing additional context that helps in filtering and ranking the results[1](https://www.promptingguide.ai/research/rag).\n",
    "\n",
    "4. **Multimodal RAG**: This involves using RAG techniques across different types of data, such as text and images. For example, embedding and storing images in a vector database allows for querying images with text, enabling more versatile and comprehensive search capabilities[2](https://github.com/NirDiamant/RAG_TECHNIQUES).\n",
    "\n",
    "\n",
    "[1](https://www.promptingguide.ai/research/rag): [Nextra](https://www.promptingguide.ai/research/rag)\n",
    "[2](https://github.com/NirDiamant/RAG_TECHNIQUES): [GitHub - NirDiamant/RAG_Techniques](https://github.com/NirDiamant/RAG_TECHNIQUES)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 1: Optimizing RAG\n",
    "\n",
    "#### 1.1 What is a Vector Database?\n",
    "\n",
    "- A vector database is a specialized tool designed to handle unstructured data represented as vectors.\n",
    "- The term \"vector database\" is somewhat misleading; these are not traditional databases but compute engines optimized for vector data.\n",
    "- In Generative AI, vector data is often referred to as vector embeddings. For RAG, vectors and embeddings are used interchangeably.\n",
    "\n",
    "#### 1.2 Understanding Vector Embeddings\n",
    "\n",
    "- Vector embeddings are long series of numbers, typically consisting of hundreds or thousands of values.\n",
    "- Generated by deep neural networks, specifically from the second-to-last layer, encapsulating learned information without the final predictive layer.\n",
    "- Embedding models generate these embeddings and vary widely in architecture and data types they can process.\n",
    "\n",
    "**Key Points:**\n",
    "  - **Data and Model Type Matching**: Match the embedding model to the data type (e.g., image models for images, text models for text).\n",
    "  - **Vector Size Consistency**: Only compare vectors of the same size, using the same embedding model for both vectorization and retrieval.\n",
    "\n",
    "#### 1.3 The Role of Large Language Models (LLMs)\n",
    "\n",
    "- LLMs, such as GPT-4, serve as the interface for interacting with your data in RAG.\n",
    "- Based on the transformer model, LLMs predict the most likely next token given a sequence of tokens.\n",
    "- Publicly available LLMs are trained on extensive datasets of publicly available information, lacking access to your specific data.\n",
    "\n",
    "#### 1.4 How RAG Works\n",
    "\n",
    "1. **Vectorization**: Data is vectorized using embedding models.\n",
    "2. **Storage**: Vectorized data is stored in a vector database.\n",
    "3. **Interaction**: An LLM interfaces with the vector database to retrieve relevant information.\n",
    "\n",
    "**Process:**\n",
    "- A question is vectorized by an embedding model.\n",
    "- The vectorized query searches the vector database for similar embeddings.\n",
    "- Relevant results are retrieved and provided as context to the LLM.\n",
    "- The LLM generates a coherent and contextually appropriate response.\n",
    "\n",
    "#### 1.5 Preprocessing for RAG\n",
    "\n",
    "- Preprocessing involves chunking, embeddings, and metadata.\n",
    "\n",
    "**Components:**\n",
    "  - **Chunking**: Breaking down large text blocks into smaller, manageable chunks.\n",
    "  - **Embeddings**: Vectors generated by embedding models, representing the semantic meaning of input data.\n",
    "  - **Metadata**: Additional data stored alongside embeddings in a vector database, critical for optimizing RAG applications.\n",
    "\n",
    "#### 1.6 Chunking Considerations\n",
    "\n",
    "- Chunking splits documents into smaller, consumable chunks.\n",
    "\n",
    "**Requirements:**\n",
    "  - **Consumable**: Fit within the context window of the embedding model and the LLM.\n",
    "  - **Coherent**: Make sense as standalone pieces of text.\n",
    "  - **Contextual**: Contain all necessary context to answer a question.\n",
    "\n",
    "**Considerations:**\n",
    "  - Chunk size, overlap, and the use of special characters to mark chunk boundaries.\n",
    "\n",
    "#### 1.7 Types of Embeddings\n",
    "\n",
    "- **Dense Embeddings**: Vectors with few zero values, typically created by machine learning models.\n",
    "- **Sparse Embeddings**: Vectors with many zero values, usually generated by algorithms. Dense embeddings are generally preferred for RAG applications.\n",
    "\n",
    "#### 1.8 Metadata in RAG\n",
    "\n",
    "- Metadata includes processing metadata (e.g., section titles, paragraph numbers) and data-related metadata (e.g., publication dates, authors).\n",
    "- Helps filter searches and ensures the LLM can interpret the retrieved data correctly.\n",
    "\n",
    "#### 1.9 Introduction to Embeddings\n",
    "\n",
    "- Before vector embeddings, comparing unstructured data was challenging.\n",
    "- Embedding models, usually deep neural networks, convert various data types (text, images, videos, audio) into vectors or vector embeddings.\n",
    "- Vectors enable quantitative comparison of unstructured data.\n",
    "\n",
    "**Critical Considerations:**\n",
    "  - **Embedding Model**: Choose the correct model for your data type (e.g., ResNet50 for images, Sentence Transformers for text, Whisper for audio).\n",
    "  - **What to Embed**: Focus on embedding text, as it is a crucial medium for AI.\n",
    "  - **How to Compare Embeddings**: Ensure embeddings are of the same size for comparison.\n",
    "\n",
    "**Picking the Right Model:**\n",
    "  - **Embedding Size**: The dimensionality of the vector, affecting computational power needed for comparison.\n",
    "  - **Model Size**: Larger models provide finer results but are more computationally expensive.\n",
    "  - **Training Data**: The dataset used to train the model influences its effectiveness (e.g., language, structure, data size).\n",
    "\n",
    "#### 1.10 Embedding Examples\n",
    "\n",
    "- **Basic Embeddings**: Directly embedding chunks of text.\n",
    "- **Small to Big**: Embedding a sentence but storing the entire paragraph for increased context.\n",
    "- **Big to Small**: Embedding a paragraph but storing individual sentences for post-processing.\n",
    "- **Non-English Embeddings**: Using models trained on non-English data for embedding.\n",
    "\n",
    "#### 1.11 Metadata\n",
    "\n",
    "- Metadata is essential for making vector databases useful, providing context and filtering capabilities.\n",
    "- **Chunking Metadata**: Information from the chunking process (e.g., sentence number, section header).\n",
    "- **Non-Chunking Metadata**: Additional information not tied to chunking (e.g., author, last update).\n",
    "\n",
    "**Storing Metadata:**\n",
    "  - Link to traditional databases or store directly in the vector store for easier and faster access."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.12 Chunking\n",
    "\n",
    "Chunking is the process of breaking large texts up into small, workable pieces.\n",
    "\n",
    "In the first block here, three things are imported: the document object, the character text splitter object, and the OS library.\n",
    "\n",
    "1. **Imports**:\n",
    "   - **Document Object**: LangChain's native way to store objects. This is used to add metadata to the text and prepare it for the vector store.\n",
    "   - **Character Text Splitter**: A LangChain object that can split strings based on some preset parameters. In this case, it is used for determining chunk size and chunk overlap.\n",
    "   - **OS Library**: Used for navigating the directory structure of the operating system.\n",
    "\n",
    "2. **Functionality**:\n",
    "   - **Set Folder**: First, ensure that the right folder is being used. In this case, the Big Star Collectibles folder is used. To access the list of text files within this folder, the OS library is used to get a list of the directory.\n",
    "   - **Create List**: Next, create an empty list object to hold all of the chunked up texts that will be created.\n",
    "   - **Loop Through Files**: Then start looping through all of the files and chunking them up. So, what is done in this loop?\n",
    "     - Start by opening up the file and reading the entire page in as a single string.\n",
    "     - Next, create a `CharacterTextSplitter` object. This specific instance is set up to split strings into 128 character chunks with 32 character overlaps.\n",
    "     - Then use the object's `split_text` function and pass the string containing the entire file through to get the chunks.\n",
    "     - The last bit of functionality in the chunking section is to loop through each of these chunked texts and create a document object from each chunk. To ensure that the chunks are kept stored in the vector store, assign it to the `page_content` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "# from langchain.document import Document\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import os\n",
    "\n",
    "# Set the folder containing the text files\n",
    "folder_path = 'Big Star Collectibles'\n",
    "\n",
    "# Get a list of text files in the folder\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "# Initialize an empty list to hold chunked texts\n",
    "chunked_texts = []\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for file_name in file_list:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    # Open and read the entire file as a single string\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Create a CharacterTextSplitter object\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=128, chunk_overlap=32)\n",
    "    \n",
    "    # Split the text into chunks\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    \n",
    "    # Loop through each chunk and create a Document object\n",
    "    for chunk in chunks:\n",
    "        doc = Document(page_content=chunk)\n",
    "        chunked_texts.append(doc)\n",
    "\n",
    "# Print the first chunk to verify\n",
    "print(chunked_texts[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.13 Metadata\n",
    "\n",
    "This section is about storing metadata with chunk data. In LangChain, metadata is stored within the document object. Previously, chunks were stored in the `page_content` parameter. To store metadata, simply add a `metadata` parameter. Metadata is stored as a dictionary, and you can define the metadata to store. Common metadata includes the title of the document and the chunk number, indicating where in the document the chunk was taken from.\n",
    "\n",
    "To implement this, enumerate through the list instead of just looping through it. This allows access to the chunk number and identifies where in the document the chunk was taken from.\n",
    "\n",
    "This code will read all text files in the specified folder, split each file into 128-character chunks with 32-character overlaps, and store each chunk as a `Document` object with metadata in the `chunked_texts` list. The last lines print the first chunk and its metadata to verify the process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from langchain.document import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import os\n",
    "\n",
    "# Set the folder containing the text files\n",
    "folder_path = 'Big Star Collectibles'\n",
    "\n",
    "# Get a list of text files in the folder\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "# Initialize an empty list to hold chunked texts\n",
    "chunked_texts = []\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for file_name in file_list:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    # Open and read the entire file as a single string\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Create a CharacterTextSplitter object\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=128, chunk_overlap=32)\n",
    "    \n",
    "    # Split the text into chunks\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    \n",
    "    # Loop through each chunk and create a Document object with metadata\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        metadata = {\n",
    "            'doc_title': file_name,\n",
    "            'chunk_number': i\n",
    "        }\n",
    "        doc = Document(page_content=chunk, metadata=metadata)\n",
    "        chunked_texts.append(doc)\n",
    "\n",
    "# Print the first chunk and its metadata to verify\n",
    "print(chunked_texts[0].page_content)\n",
    "print(chunked_texts[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.14 Embed and Store\n",
    "\n",
    "With the data chunked and some metadata saved in each of the chunked objects, the next step is to embed the data and store it into a vector database. This involves two new imports: FAISS and HuggingFaceEmbeddings.\n",
    "\n",
    "1. **Imports**:\n",
    "   - **FAISS**: Stands for Facebook AI Similarity Search. This library is the foundation for many popular AI-native vector databases.\n",
    "   - **HuggingFaceEmbeddings**: Imported from `langchain_community`. Initially, LangChain had numerous integrations, but as it grew, many of these integrations moved to the LangChain community library, including HuggingFaceEmbeddings.\n",
    "\n",
    "2. **Embedding and Storing**:\n",
    "   - **Import Libraries**: Start by importing the FAISS library from LangChain and the HuggingFaceEmbeddings from the community module.\n",
    "   - **Instantiate Embedding Function**: Create an instance of the HuggingFaceEmbeddings object. The default embedding model is `all-mpnet-base-v2`, which has 768 dimensions. Only vectors of the same dimensionality can be compared.\n",
    "   - **Create Vector Store**: Use the documents created in the metadata and chunking steps. Pass the embedding function and the documents to create the vector store.\n",
    "\n",
    "This code will read all text files in the specified folder, split each file into 128-character chunks with 32-character overlaps (each chunk starts 32 characters after the start of the previous chunk, creating an overlap), store each chunk as a `Document` object with metadata, and then embed and store these documents into a FAISS vector database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import os\n",
    "\n",
    "# Set the folder containing the text files\n",
    "folder_path = 'Big Star Collectibles'\n",
    "\n",
    "# Get a list of text files in the folder\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "# Initialize an empty list to hold chunked texts\n",
    "chunked_texts = []\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for file_name in file_list:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    # Open and read the entire file as a single string\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Create a CharacterTextSplitter object\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=128, chunk_overlap=32)\n",
    "    \n",
    "    # Split the text into chunks\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    \n",
    "    # Loop through each chunk and create a Document object with metadata\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        metadata = {\n",
    "            'doc_title': file_name,\n",
    "            'chunk_number': i\n",
    "        }\n",
    "        doc = Document(page_content=chunk, metadata=metadata)\n",
    "        chunked_texts.append(doc)\n",
    "\n",
    "# Instantiate the HuggingFaceEmbeddings object\n",
    "embedding_function = HuggingFaceEmbeddings()\n",
    "\n",
    "# Create the vector store\n",
    "vector_store = FAISS.from_documents(chunked_texts, embedding_function)\n",
    "\n",
    "# Print the first document's embedding to verify\n",
    "print(vector_store.index[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.15 Querying\n",
    "\n",
    "Querying the vector database is part of what goes on behind the scenes in a RAG application. The LLM queries the `vector_store` to get some context back to create a response. When interacting with the RAG app, this query is not visible. This section provides a peek behind the scenes to see what the LLM sees.\n",
    "\n",
    "When querying a vector database, some top_k results are returned. For LangChain FAISS, the default k is 4. \n",
    "\n",
    "1. **Prepare the Vector Store**:\n",
    "   - **as_retriever Function**: The first step to perform a `vector_store` query in LangChain is to take the `vector_store` and call the `as_retriever` function on it. This prepares the `vector_store` to be queried with strings and abstracts out the necessity of turning a string into an embedding and calling a query function directly.\n",
    "\n",
    "2. **Perform the Query**:\n",
    "   - **invoke Function**: Call the `invoke` function of the retriever and pass a string. The result is the top four results in the `vector_store` according to the embedding model defined earlier.\n",
    "\n",
    "This code will read all text files in the specified folder, split each file into 128-character chunks with 32-character overlaps, store each chunk as a `Document` object with metadata, embed and store these documents into a FAISS vector database, and then query the vector store to retrieve the top 4 results based on the query string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the vector store for querying\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# Perform a query\n",
    "query = \"example query text\"\n",
    "results = retriever.invoke(query)\n",
    "\n",
    "# Print the top 4 results\n",
    "for result in results:\n",
    "    print(result.page_content)\n",
    "    print(result.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.15 Adding the LLM\n",
    "\n",
    "The final part of creating a RAG application on top of the vector store is to add the LLM (Large Language Model). For this part, access to an LLM is needed. This can be done using an API key from providers like OctoAI, OpenAI, or others, or by running an LLM locally. \n",
    "\n",
    "1. **Set Up LLM Access**:\n",
    "   - **Import Environment Variables**: Use Python-dotenv's `load_dotenv` method to load environment variables.\n",
    "   - **Import OpenAI**: Import OpenAI from `langchain_openai` and initialize it as the LLM.\n",
    "\n",
    "2. **Create a Prompt Template**:\n",
    "   - **Prompt Creation**: Create a prompt template for the chat. Use brackets to pass the question and context, similar to using an f-string in Python.\n",
    "   - **ChatPromptTemplate**: Use the `ChatPromptTemplate` object from LangChain to create the prompt template.\n",
    "\n",
    "3. **Create the Chain**:\n",
    "   - **Imports**: Import `RunnablePassthrough` and `StrOutputParser`.\n",
    "     - **RunnablePassthrough**: Takes a string and treats it as a function by passing the string through the function.\n",
    "     - **StrOutputParser**: Parses the output of the chain as a string.\n",
    "   - **Build the Chain**: \n",
    "     - Get the context and the question using the objects mentioned.\n",
    "     - Pass these to the prompt created.\n",
    "     - Pass the prompt to the LLM.\n",
    "     - Pipe the output of the LLM to the `StrOutputParser`.\n",
    "\n",
    "4. **Invoke the Chain**:\n",
    "   - **Get a Response**: Invoke the chain to get a response. The response will combine the queries and return a single string from the given context.\n",
    "\n",
    "This code will read all text files in the specified folder, split each file into 128-character chunks with 32-character overlaps, store each chunk as a `Document` object with metadata, embed and store these documents into a FAISS vector database, and then query the vector store and use the LLM to get a response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import RunnablePassthrough, StrOutputParser\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set the folder containing the text files\n",
    "folder_path = 'Big Star Collectibles'\n",
    "\n",
    "# Get a list of text files in the folder\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "# Initialize an empty list to hold chunked texts\n",
    "chunked_texts = []\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for file_name in file_list:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    # Open and read the entire file as a single string\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Create a CharacterTextSplitter object\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=128, chunk_overlap=32)\n",
    "    \n",
    "    # Split the text into chunks\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    \n",
    "    # Loop through each chunk and create a Document object with metadata\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        metadata = {\n",
    "            'doc_title': file_name,\n",
    "            'chunk_number': i\n",
    "        }\n",
    "        doc = Document(page_content=chunk, metadata=metadata)\n",
    "        chunked_texts.append(doc)\n",
    "\n",
    "# Instantiate the HuggingFaceEmbeddings object\n",
    "embedding_function = HuggingFaceEmbeddings()\n",
    "\n",
    "# Create the vector store\n",
    "vector_store = FAISS.from_documents(chunked_texts, embedding_function)\n",
    "\n",
    "# Prepare the vector store for querying\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# Instantiate the OpenAI LLM\n",
    "llm = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Create a prompt template\n",
    "prompt_template = ChatPromptTemplate.from_template(\"Question: {question}\\nContext: {context}\")\n",
    "\n",
    "# Create the chain\n",
    "chain = RunnablePassthrough(\n",
    "    retriever,\n",
    "    prompt_template,\n",
    "    llm,\n",
    "    StrOutputParser()\n",
    ")\n",
    "\n",
    "# Perform a query and get a response\n",
    "query = \"example query text\"\n",
    "response = chain.invoke(query)\n",
    "\n",
    "# Print the response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "How can you modify a Retrieval-Augmented Generation (RAG) application to cite its document sources using metadata stored in a vector database?\n",
    "\n",
    "### Answer: Cite Your Document Sources\n",
    "To modify a RAG application to cite its document sources, follow these steps:\n",
    "\n",
    "1. **Store Document Metadata**: Ensure that the names of the documents are stored as part of the metadata in the vector store. This allows access to this information when retrieving objects from the vector store.\n",
    "\n",
    "2. **Retrieve Metadata**: When retrieving objects from the vector store, access this metadata to get the document names.\n",
    "\n",
    "3. **Prompt Engineering**: Modify the prompt to instruct the language model to cite its sources. This can be done by adding a simple sentence to the prompt, such as \"Please cite your sources.\"\n",
    "\n",
    "### Example Implementation\n",
    "In this example:\n",
    "- The `VectorStore` class is used to interact with the vector database.\n",
    "- The `retrieve_and_cite` function retrieves documents related to the query and extracts their names from the metadata.\n",
    "- The prompt is modified to include an instruction to cite sources.\n",
    "- The language model generates a response, and the sources are appended to the response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### retrieve_and_cite function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import VectorStore\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Initialize the vector store and language model\n",
    "vector_store = VectorStore()\n",
    "llm = OpenAI()\n",
    "\n",
    "# Function to retrieve documents and cite sources\n",
    "def retrieve_and_cite(query):\n",
    "    # Retrieve documents from the vector store\n",
    "    results = vector_store.similarity_search(query)\n",
    "    \n",
    "    # Extract document names from metadata\n",
    "    sources = [result.metadata['document_name'] for result in results]\n",
    "    \n",
    "    # Create a prompt with the query and instruction to cite sources\n",
    "    prompt = f\"{query}\\n\\nCite your sources.\"\n",
    "    \n",
    "    # Get the response from the language model\n",
    "    response = llm(prompt)\n",
    "    \n",
    "    # Append the sources to the response\n",
    "    cited_response = f\"{response}\\n\\nSources: {', '.join(sources)}\"\n",
    "    \n",
    "    return cited_response\n",
    "\n",
    "# Example usage\n",
    "query = \"Explain the benefits of RAG applications.\"\n",
    "response = retrieve_and_cite(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cite Your Document Sources Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cite Your Document Sources\n",
    "# Import necessary libraries\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import RunnablePassthrough, StrOutputParser\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set the folder containing the text files\n",
    "folder_path = 'Big Star Collectibles'\n",
    "\n",
    "# Get a list of text files in the folder\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "# Initialize an empty list to hold chunked texts\n",
    "chunked_texts = []\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for file_name in file_list:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    # Open and read the entire file as a single string\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Create a CharacterTextSplitter object\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=128, chunk_overlap=32)\n",
    "    \n",
    "    # Split the text into chunks\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    \n",
    "    # Loop through each chunk and create a Document object with metadata\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        metadata = {\n",
    "            'doc_title': file_name,\n",
    "            'chunk_number': i\n",
    "        }\n",
    "        doc = Document(page_content=chunk, metadata=metadata)\n",
    "        chunked_texts.append(doc)\n",
    "\n",
    "# Instantiate the HuggingFaceEmbeddings object\n",
    "embedding_function = HuggingFaceEmbeddings()\n",
    "\n",
    "# Create the vector store\n",
    "vector_store = FAISS.from_documents(chunked_texts, embedding_function)\n",
    "\n",
    "# Prepare the vector store for querying\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# Instantiate the OpenAI LLM\n",
    "llm = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Create a prompt template with source citation\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"Question: {question}\\nContext: {context}\\nPlease cite your sources.\"\n",
    ")\n",
    "\n",
    "# Create the chain\n",
    "chain = RunnablePassthrough(\n",
    "    retriever,\n",
    "    prompt_template,\n",
    "    llm,\n",
    "    StrOutputParser()\n",
    ")\n",
    "\n",
    "# Perform a query and get a response\n",
    "query = \"example query text\"\n",
    "response = chain.invoke(query)\n",
    "\n",
    "# Print the response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The retrieve_and_cite function and the \"Cite Your Document Sources\" code serve similar purposes but are structured differently. The retrieve_and_cite function is a simpler example to illustrate the basic concept of retrieving documents and citing sources.\n",
    "\n",
    "On the other hand, the \"Cite Your Document Sources\" code is a more comprehensive implementation that includes additional steps like text splitting, embedding generation, and creating a query chain. This code is designed to handle more complex scenarios and provides a more robust solution.\n",
    "\n",
    "If you prefer to use the retrieve_and_cite function within the comprehensive implementation, you can integrate it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.document import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import RunnablePassthrough, StrOutputParser\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set the folder containing the text files\n",
    "folder_path = 'Big Star Collectibles'\n",
    "\n",
    "# Get a list of text files in the folder\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "# Initialize an empty list to hold chunked texts\n",
    "chunked_texts = []\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for file_name in file_list:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    # Open and read the entire file as a single string\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Create a CharacterTextSplitter object\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=128, chunk_overlap=32)\n",
    "    \n",
    "    # Split the text into chunks\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    \n",
    "    # Loop through each chunk and create a Document object with metadata\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        metadata = {\n",
    "            'doc_title': file_name,\n",
    "            'chunk_number': i\n",
    "        }\n",
    "        doc = Document(page_content=chunk, metadata=metadata)\n",
    "        chunked_texts.append(doc)\n",
    "\n",
    "# Instantiate the HuggingFaceEmbeddings object\n",
    "embedding_function = HuggingFaceEmbeddings()\n",
    "\n",
    "# Create the vector store\n",
    "vector_store = FAISS.from_documents(chunked_texts, embedding_function)\n",
    "\n",
    "# Prepare the vector store for querying\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# Instantiate the OpenAI LLM\n",
    "llm = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Function to retrieve documents and cite sources\n",
    "def retrieve_and_cite(query):\n",
    "    # Retrieve documents from the vector store\n",
    "    results = retriever.retrieve(query)\n",
    "    \n",
    "    # Extract document names from metadata\n",
    "    sources = [result.metadata['doc_title'] for result in results]\n",
    "    \n",
    "    # Create a prompt with the query and instruction to cite sources\n",
    "    prompt = f\"{query}\\n\\nPlease cite your sources.\"\n",
    "    \n",
    "    # Get the response from the language model\n",
    "    response = llm(prompt)\n",
    "    \n",
    "    # Append the sources to the response\n",
    "    cited_response = f\"{response}\\n\\nSources: {', '.join(sources)}\"\n",
    "    \n",
    "    return cited_response\n",
    "\n",
    "# Example usage\n",
    "query = \"Explain the benefits of RAG applications.\"\n",
    "response = retrieve_and_cite(query)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change the Chunk Size\n",
    "\n",
    "When using the `CharacterTextSplitter`, there are two parameters that are automatically set: `separator` and `is_separator_regex`. \n",
    "By default, the `separator` parameter is set to a double new line (`\\n\\n`). This means that if the text does not contain double new lines, it may not form a new chunk, even if it exceeds the specified chunk size.\n",
    "\n",
    "To ensure that chunks are formed around the correct chunk size, a custom separator can be defined. In this case, a single new line (`\\n`) can be used as the custom separator. This will help in forming chunks of the correct size and overlap.\n",
    "\n",
    "### How to modify the code to include a custom separator and add the question to the prompt?\n",
    "\n",
    "This code will ensure that the text is split into chunks of the correct size using a single new line as the separator. It also includes the question in the prompt to instruct the LLM to cite its sources, ensuring that the LLM provides the necessary citations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from langchain.document import Document\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "import os\n",
    "\n",
    "# Set the folder containing the text files\n",
    "folder_path = 'Big Star Collectibles'\n",
    "\n",
    "# Get a list of text files in the folder\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "# Initialize an empty list to hold chunked texts\n",
    "chunked_texts = []\n",
    "\n",
    "# Loop through each file in the folder\n",
    "for file_name in file_list:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    \n",
    "    # Open and read the entire file as a single string\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    # Create a CharacterTextSplitter object with a custom separator\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=128, chunk_overlap=32, separator='\\n')\n",
    "    \n",
    "    # Split the text into chunks\n",
    "    chunks = text_splitter.split_text(text)\n",
    "    \n",
    "    # Loop through each chunk and create a Document object with metadata\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        metadata = {\n",
    "            'doc_title': file_name,\n",
    "            'chunk_number': i\n",
    "        }\n",
    "        doc = Document(page_content=chunk, metadata=metadata)\n",
    "        chunked_texts.append(doc)\n",
    "\n",
    "# Print the first chunk to verify\n",
    "print(chunked_texts[0].page_content)\n",
    "\n",
    "# Import necessary libraries for embedding and querying\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import RunnablePassthrough, StrOutputParser\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Instantiate the HuggingFaceEmbeddings object\n",
    "embedding_function = HuggingFaceEmbeddings()\n",
    "\n",
    "# Create the vector store\n",
    "vector_store = FAISS.from_documents(chunked_texts, embedding_function)\n",
    "\n",
    "# Prepare the vector store for querying\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# Instantiate the OpenAI LLM\n",
    "llm = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Create a prompt template with source citation\n",
    "prompt_template = ChatPromptTemplate.from_template(\n",
    "    \"Question: {question}\\nContext: {context}\\nPlease cite your sources.\"\n",
    ")\n",
    "\n",
    "# Create the chain\n",
    "chain = RunnablePassthrough(\n",
    "    retriever,\n",
    "    prompt_template,\n",
    "    llm,\n",
    "    StrOutputParser()\n",
    ")\n",
    "\n",
    "# Perform a query and get a response\n",
    "query = \"example query text\"\n",
    "response = chain.invoke(query)\n",
    "\n",
    "# Print the response\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 2: Vector Embeddings for Images\n",
    "\n",
    "Images are one of the classic unstructured data types, and vector embeddings are crucial for comparing images. There are two main types of vectors used for this purpose: semantic vectors and visual vectors. These vectors describe images in fundamentally different ways.\n",
    "\n",
    "### 2.1 Semantic Embeddings\n",
    "\n",
    "Semantic embeddings describe the meaning of the image. They are derived from deep learning models, where image data passes from the input layer, through a series of hidden layers, to an output layer. The second-to-last layer captures all the meaning that the model has derived to make its prediction, classification, or segmentation. This layer is used as the semantic embedding. These embeddings are typically used in Retrieval-Augmented Generation (RAG) applications because they capture the essence of what the image represents.\n",
    "\n",
    "### 2.2 Visual or Pixel Embeddings\n",
    "\n",
    "Visual embeddings encode what the image literally looks like. In image models trained in frameworks like PyTorch, images are compressed into a vector as the input to the model. These vectors capture the visual appearance of the image. While technically vector embeddings, visual embeddings are less commonly used in RAG applications compared to semantic embeddings.\n",
    "\n",
    "In summary, for RAG applications, the focus is primarily on semantic embeddings because they provide a meaningful representation of the image's content.\n",
    "\n",
    "### 2.3 Vision Models 101\n",
    "\n",
    "To understand how machines compare images, it's essential to grasp how vision models work. Vision models are a type of deep neural network trained for computer vision tasks such as image classification, segmentation, and object detection.\n",
    "\n",
    "#### 2.3.1 History of Neural Networks\n",
    "\n",
    "The first neural networks were simple, modeled as layers of neurons fully connected to adjacent layers. Over time, different neural network architectures were developed to better handle various types of data.\n",
    "\n",
    "#### 2.3.2 Convolutional Neural Networks (CNNs)\n",
    "\n",
    "CNNs are a class of deep neural networks that have proven very effective for image processing tasks.\n",
    "\n",
    "- **Convolution**: A technique that helps in getting local context from images. It involves a filter that moves across the image, performing element-wise multiplication and summing the results to produce a new image.\n",
    "- **Pooling**: Max pooling is a method to downsample the image by taking the maximum value in a region, which helps in reducing the dimensionality while retaining important features.\n",
    "- **Architecture**: CNNs combine convolutional layers and pooling layers to extract and combine local contexts from different parts of the image.\n",
    "\n",
    "#### 2.3.3 Vision Transformers (ViTs)\n",
    "\n",
    "Vision transformers apply the attention mechanism from transformer models (originally used for language) to computer vision.\n",
    "\n",
    "- **Patches**: Images are divided into N by N patches, each turned into embeddings.\n",
    "- **Processing**: These embeddings, combined with a class token and positional encoding, are fed into the transformer. The output is processed by a multilayer perceptron (MLP) to produce logits for tasks like object detection or segmentation.\n",
    "\n",
    "Here's a representation of how a convolution works:\n",
    "\n",
    "- **Convolution**: Imagine a 2D image filled with numbers. A 3x3 filter moves across the image, performing element-wise multiplication with the image values and summing the results to produce a new image.\n",
    "- **Pooling**: In max pooling, the maximum value in a region (e.g., 12, 20, 8, 12) is taken to represent that region (e.g., 20).\n",
    "\n",
    "Vision transformers operate similarly but use patches and attention mechanisms to process the image.\n",
    "\n",
    "### 2.4 Getting Semantic Vectors\n",
    "\n",
    "In this section, we will obtain a semantic vector from an image using the OpenCLIP embeddings with LangChain.\n",
    "\n",
    "1. **Import Libraries**:\n",
    "   - `OpenCLIPEmbeddings` from LangChain is used to load the embedding model.\n",
    "   - `glob` is used to retrieve all image file paths in the specified directory.\n",
    "\n",
    "2. **Get Image File Paths**:\n",
    "   - `glob.glob` retrieves all `.jpg` files in the specified folder.\n",
    "\n",
    "3. **Load Embedding Model**:\n",
    "   - `OpenCLIPEmbeddings` is instantiated to create the embedding model.\n",
    "\n",
    "4. **Embed Images**:\n",
    "   - The `embed_image` function is called with the list of image file paths to generate embeddings.\n",
    "\n",
    "5. **Examine Embeddings**:\n",
    "   - The first embedding is printed to see what it looks like.\n",
    "   - The length of the first embedding is printed to check its dimensionality, which should be 1024.\n",
    "\n",
    "This process will generate semantic vectors for the images, which can then be used for various tasks such as image comparison or retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Vision Models 101\n",
    "\n",
    "To understand how machines compare images, it's essential to grasp how vision models work. Vision models are a type of deep neural network trained for computer vision tasks such as image classification, segmentation, and object detection.\n",
    "\n",
    "1. **History of Neural Networks**:\n",
    "   - **1960s**: The first neural networks were simple, modeled as layers of neurons fully connected to adjacent layers.\n",
    "   - **Progression**: Over time, different neural network architectures were developed to better handle various types of data.\n",
    "\n",
    "2. **Convolutional Neural Networks (CNNs)**:\n",
    "   - **Convolution**: A technique that helps in getting local context from images. It involves a filter that moves across the image, performing element-wise multiplication and summing the results to produce a new image.\n",
    "   - **Pooling**: Max pooling is a method to downsample the image by taking the maximum value in a region, which helps in reducing the dimensionality while retaining important features.\n",
    "   - **Architecture**: CNNs combine convolutional layers and pooling layers to extract and combine local contexts from different parts of the image.\n",
    "\n",
    "3. **Vision Transformers (ViTs)**:\n",
    "   - **Introduction**: Vision transformers apply the attention mechanism from transformer models (originally used for language) to computer vision.\n",
    "   - **Patches**: Images are divided into N by N patches, each turned into embeddings.\n",
    "   - **Processing**: These embeddings, combined with a class token and positional encoding, are fed into the transformer. The output is processed by a multilayer perceptron (MLP) to produce logits for tasks like object detection or segmentation.\n",
    "\n",
    "Here's a visual representation of how a convolution works:\n",
    "\n",
    "- **Convolution**: Imagine a 2D image filled with numbers. A 3x3 filter moves across the image, performing element-wise multiplication with the image values and summing the results to produce a new image.\n",
    "- **Pooling**: In max pooling, the maximum value in a region (e.g., 12, 20, 8, 12) is taken to represent that region (e.g., 20).\n",
    "\n",
    "Vision transformers operate similarly but use patches and attention mechanisms to process the image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Getting Semantic Vectors\n",
    "\n",
    "In this section, a semantic vector from an image using the OpenCLIP embeddings with LangChain will obtain. \n",
    "\n",
    "1. **Import Libraries**:\n",
    "   - `OpenCLIPEmbeddings` from LangChain is used to load the embedding model.\n",
    "   - `glob` is used to retrieve all image file paths in the specified directory.\n",
    "\n",
    "2. **Get Image File Paths**:\n",
    "   - `glob.glob` retrieves all `.jpg` files in the specified folder.\n",
    "\n",
    "3. **Load Embedding Model**:\n",
    "   - `OpenCLIPEmbeddings` is instantiated to create the embedding model.\n",
    "\n",
    "4. **Embed Images**:\n",
    "   - `embed_image` function is called with the list of image file paths to generate embeddings.\n",
    "\n",
    "5. **Examine Embeddings**:\n",
    "   - The first embedding is printed to see what it looks like.\n",
    "   - The length of the first embedding is printed to check its dimensionality, which should be 1024.\n",
    "\n",
    "This process will generate semantic vectors for the images, which can then be used for various tasks such as image comparison or retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from langchain.embeddings import OpenCLIPEmbeddings\n",
    "import glob\n",
    "\n",
    "# Get all image file paths\n",
    "image_folder_path = 'path/to/your/images'\n",
    "image_file_paths = glob.glob(f\"{image_folder_path}/*.jpg\")\n",
    "\n",
    "# Load the OpenCLIP embedding model\n",
    "embedding_model = OpenCLIPEmbeddings()\n",
    "\n",
    "# Embed the images\n",
    "embeddings = embedding_model.embed_image(image_file_paths)\n",
    "\n",
    "# Examine the first embedding\n",
    "print(embeddings[0])\n",
    "\n",
    "# Check the dimensionality of the embeddings\n",
    "print(len(embeddings[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Storing Image Vectors\n",
    "\n",
    "This section explains how to store image vectors, similar to the process used in the text RAG chapter. Here are the steps:\n",
    "\n",
    "1. **Imports**:\n",
    "   - **LangChain Imports**: `Document`, `FAISS`, and `OpenCLIPEmbeddings`.\n",
    "   - **Other Imports**: `glob` for handling multiple file paths, and `base64` for converting images into base64 strings for the LLM. base64 encoding is used to convert images into text-based strings, ensuring compatibility with text-based protocols and ease of embedding within documents. This allows seamless integration of image data into workflows that primarily handle text.\n",
    "\n",
    "2. **Initialize List**:\n",
    "   - Create an empty list to hold the documents.\n",
    "\n",
    "3. **Define the Encode Image Function**:\n",
    "   - This function takes a file path, opens the file, reads it as bytes, and returns a UTF-encoded string of the file. A UTF-encoded string of a file is a text representation of the file's binary data, encoded using a Unicode Transformation Format (UTF), such as UTF-8. This encoding ensures that the binary data can be safely and consistently represented as text, making it easier to store, transmit, and process across different systems and platforms.\n",
    "\n",
    "4. **Process Image Paths**:\n",
    "   - Loop through each image path, encode the image, and create a document containing the encoded image and metadata (including the image path).\n",
    "\n",
    "5. **Store in FAISS Vector Database**:\n",
    "   - Use `OpenCLIPEmbeddings` as the embedding function to store the documents in a FAISS vector database.\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "1. **Imports**:\n",
    "   - `Document`, `FAISS`, and `OpenCLIPEmbeddings` from LangChain.\n",
    "   - `glob` for retrieving image file paths.\n",
    "   - `base64` for encoding images.\n",
    "\n",
    "2. **Get Image File Paths**:\n",
    "   - Use `glob.glob` to retrieve all `.jpg` files in the specified folder.\n",
    "\n",
    "3. **Initialize List**:\n",
    "   - Create an empty list `documents` to hold the encoded images and their metadata.\n",
    "\n",
    "4. **Encode Image Function**:\n",
    "   - The `encode_image` function reads the image file as bytes and encodes it to a base64 string.\n",
    "\n",
    "5. **Create Documents**:\n",
    "   - Loop through each image path, encode the image, and create a `Document` object with the encoded image and metadata.\n",
    "\n",
    "6. **Create Vector Store**:\n",
    "   - Use `OpenCLIPEmbeddings` to create the FAISS vector store from the documents.\n",
    "\n",
    "This code will store the image vectors in a FAISS vector database, making them ready for retrieval and comparison. The explanation provided clarifies each step, ensuring a clear understanding of the process and its purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from langchain.document import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenCLIPEmbeddings\n",
    "import glob\n",
    "import base64\n",
    "\n",
    "# Get all image file paths\n",
    "image_folder_path = 'path/to/your/images'\n",
    "image_file_paths = glob.glob(f\"{image_folder_path}/*.jpg\")\n",
    "\n",
    "# Initialize an empty list to hold documents\n",
    "documents = []\n",
    "\n",
    "# Define a function to encode images\n",
    "def encode_image(file_path):\n",
    "    with open(file_path, 'rb') as image_file:\n",
    "        image_bytes = image_file.read()\n",
    "    return base64.b64encode(image_bytes).decode('utf-8')\n",
    "\n",
    "# Loop through each image path and create a document\n",
    "for file_path in image_file_paths:\n",
    "    encoded_image = encode_image(file_path)\n",
    "    metadata = {'image_path': file_path}\n",
    "    doc = Document(page_content=encoded_image, metadata=metadata)\n",
    "    documents.append(doc)\n",
    "\n",
    "# Load the OpenCLIP embedding model\n",
    "embedding_model = OpenCLIPEmbeddings()\n",
    "\n",
    "# Create the FAISS vector store\n",
    "vector_store = FAISS.from_documents(documents, embedding_model)\n",
    "\n",
    "# Print the first document to verify\n",
    "print(documents[0].page_content)\n",
    "print(documents[0].metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Comparing Images Semantically\n",
    "\n",
    "Now that the data is stored in a vector database, the next step is to compare images to find the most similar sets. Here's how to do it:\n",
    "\n",
    "1. **Turn Vector Store into a Retriever**:\n",
    "   - Convert the vector store into a retriever to facilitate querying. Converting the vector store into a retriever means transforming the stored vectors into a format or system that allows efficient searching and querying. This enables you to quickly find and retrieve relevant vectors (e.g., images or documents) based on similarity or specific criteria, facilitating tasks like information retrieval and comparison.\n",
    "\n",
    "   ```python\n",
    "   # Prepare the vector store for querying\n",
    "   retriever = vector_store.as_retriever()\n",
    "   ```\n",
    "\n",
    "2. **Retrieve Images**:\n",
    "   - Use the retriever to get images by passing the encoded string of an image.\n",
    "   - For example, pass the encoded string of the first cat image to retrieve the top four most similar images.\n",
    "\n",
    "   ```python\n",
    "   # Encode the first image to use as a query\n",
    "   query_image_path = image_file_paths[0]\n",
    "   query_image_encoded = encode_image(query_image_path)\n",
    "\n",
    "   # Perform a query to find similar images\n",
    "   results = retriever.invoke(query_image_encoded)\n",
    "\n",
    "   # Print the top 4 results\n",
    "   for result in results:\n",
    "      print(result.page_content)\n",
    "      print(result.metadata)\n",
    "   ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenCLIPEmbeddings\n",
    "from langchain.document import Document\n",
    "import glob\n",
    "import base64\n",
    "\n",
    "# Get all image file paths\n",
    "image_folder_path = 'path/to/your/images'\n",
    "image_file_paths = glob.glob(f\"{image_folder_path}/*.jpg\")\n",
    "\n",
    "# Initialize an empty list to hold documents\n",
    "documents = []\n",
    "\n",
    "# Define a function to encode images\n",
    "def encode_image(file_path):\n",
    "    with open(file_path, 'rb') as image_file:\n",
    "        image_bytes = image_file.read()\n",
    "    return base64.b64encode(image_bytes).decode('utf-8')\n",
    "\n",
    "# Loop through each image path and create a document\n",
    "for file_path in image_file_paths:\n",
    "    encoded_image = encode_image(file_path)\n",
    "    metadata = {'image_path': file_path}\n",
    "    doc = Document(page_content=encoded_image, metadata=metadata)\n",
    "    documents.append(doc)\n",
    "\n",
    "# Load the OpenCLIP embedding model\n",
    "embedding_model = OpenCLIPEmbeddings()\n",
    "\n",
    "# Create the FAISS vector store\n",
    "vector_store = FAISS.from_documents(documents, embedding_model)\n",
    "\n",
    "# Prepare the vector store for querying\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# Encode the first image to use as a query\n",
    "query_image_path = image_file_paths[0]\n",
    "query_image_encoded = encode_image(query_image_path)\n",
    "\n",
    "# Perform a query to find similar images\n",
    "results = retriever.invoke(query_image_encoded)\n",
    "\n",
    "# Print the top 4 results\n",
    "for result in results:\n",
    "    print(result.page_content)\n",
    "    print(result.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Challenge: Find the Dog Most Similar to a Cat\n",
    "\n",
    "To find the dog that looks most similar to the cats using semantic embeddings, follow these steps:\n",
    "\n",
    "1. **Prepare the Environment**:\n",
    "   - Ensure all vectors are stored in the vector store.\n",
    "   - Have the retriever ready to query the vector store.\n",
    "\n",
    "   ```python\n",
    "   # Prepare the vector store for querying\n",
    "   retriever = vector_store.as_retriever()\n",
    "   ```\n",
    "\n",
    "2. **Get Dog Image Paths**:\n",
    "   - Create a list of paths for the dog images.\n",
    "\n",
    "   ```python\n",
    "   # Separate dog and cat image paths\n",
    "   dog_image_paths = [path for path in image_file_paths if 'dog' in path]\n",
    "   cat_image_paths = [path for path in image_file_paths if 'cat' in path]\n",
    "   ```\n",
    "\n",
    "3. **Create a Dictionary for Mapping**:\n",
    "   - Map the paths of the dogs to the paths of the cats in an inversely-weighted order.\n",
    "\n",
    "4. **Retrieve Top Images for Each Dog**:\n",
    "   - For each dog image, retrieve the top four most similar images based on the Base64 encoding of the dog image.\n",
    "   - Initialize a counter (`cats_retrieved`) to zero.\n",
    "\n",
    "   ```python\n",
    "   # Function to find the dog most similar to cats\n",
    "   def find_most_similar_dog(dog_paths, retriever):\n",
    "      cat_scores = {}\n",
    "      for dog_path in dog_paths:\n",
    "         encoded_dog_image = encode_image(dog_path)\n",
    "         results = retriever.invoke(encoded_dog_image)\n",
    "         cats_retrieved = 0\n",
    "         for i, result in enumerate(results):\n",
    "               if 'cat' in result.metadata['image_path']:\n",
    "                  cats_retrieved += (4 - i)\n",
    "         cat_scores[dog_path] = cats_retrieved\n",
    "      most_similar_dog = max(cat_scores, key=cat_scores.get)\n",
    "      return most_similar_dog, cat_scores[most_similar_dog]\n",
    "   ```\n",
    "\n",
    "5. **Calculate Cat Scores**:\n",
    "   - Loop through the enumerated list of returned documents.\n",
    "   - Check if the word \"cat\" is in the source returned.\n",
    "   - Add `4 - i` (inverse weight based on rank) to `cats_retrieved` if the image is a cat.\n",
    "\n",
    "6. **Determine the Most Similar Dog**:\n",
    "   - Attach the cat score to each dog image.\n",
    "   - Identify the dog with the highest cat score.\n",
    "\n",
    "   ```python\n",
    "   # Find the dog most similar to cats\n",
    "   most_similar_dog, score = find_most_similar_dog(dog_image_paths, retriever)\n",
    "\n",
    "   # Print the result\n",
    "   print(f\"The dog most similar to cats is: {most_similar_dog} with a score of {score}\")\n",
    "   ```\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "1. **Imports**:\n",
    "   - `Document`, `FAISS`, and `OpenCLIPEmbeddings` from LangChain.\n",
    "   - `glob` for retrieving image file paths.\n",
    "   - `base64` for encoding images.\n",
    "\n",
    "2. **Get Image File Paths**:\n",
    "   - `glob.glob` retrieves all `.jpg` files in the specified folder.\n",
    "   - Separate paths for dog and cat images.\n",
    "\n",
    "3. **Initialize List**:\n",
    "   - An empty list `documents` is created to hold the encoded images and their metadata.\n",
    "\n",
    "4. **Encode Image Function**:\n",
    "   - `encode_image` function reads the image file as bytes and encodes it to a base64 string.\n",
    "\n",
    "5. **Create Documents**:\n",
    "   - Loop through each image path, encode the image, and create a `Document` object with the encoded image and metadata.\n",
    "\n",
    "6. **Create Vector Store**:\n",
    "   - Use `OpenCLIPEmbeddings` to create the FAISS vector store from the documents.\n",
    "\n",
    "7. **Prepare Retriever**:\n",
    "   - Convert the vector store into a retriever.\n",
    "\n",
    "8. **Find Most Similar Dog**:\n",
    "   - For each dog image, retrieve the top four most similar images.\n",
    "   - Calculate the cat score based on the presence of cat images in the results.\n",
    "   - Identify the dog with the highest cat score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import OpenCLIPEmbeddings\n",
    "from langchain.document import Document\n",
    "import glob\n",
    "import base64\n",
    "\n",
    "# Get all image file paths\n",
    "image_folder_path = 'path/to/your/images'\n",
    "image_file_paths = glob.glob(f\"{image_folder_path}/*.jpg\")\n",
    "\n",
    "# Separate dog and cat image paths\n",
    "dog_image_paths = [path for path in image_file_paths if 'dog' in path]\n",
    "cat_image_paths = [path for path in image_file_paths if 'cat' in path]\n",
    "\n",
    "# Initialize an empty list to hold documents\n",
    "documents = []\n",
    "\n",
    "# Define a function to encode images\n",
    "def encode_image(file_path):\n",
    "    with open(file_path, 'rb') as image_file:\n",
    "        image_bytes = image_file.read()\n",
    "    return base64.b64encode(image_bytes).decode('utf-8')\n",
    "\n",
    "# Loop through each image path and create a document\n",
    "for file_path in image_file_paths:\n",
    "    encoded_image = encode_image(file_path)\n",
    "    metadata = {'image_path': file_path}\n",
    "    doc = Document(page_content=encoded_image, metadata=metadata)\n",
    "    documents.append(doc)\n",
    "\n",
    "# Load the OpenCLIP embedding model\n",
    "embedding_model = OpenCLIPEmbeddings()\n",
    "\n",
    "# Create the FAISS vector store\n",
    "vector_store = FAISS.from_documents(documents, embedding_model)\n",
    "\n",
    "# Prepare the vector store for querying\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# Function to find the dog most similar to cats\n",
    "def find_most_similar_dog(dog_paths, retriever):\n",
    "    cat_scores = {}\n",
    "    for dog_path in dog_paths:\n",
    "        encoded_dog_image = encode_image(dog_path)\n",
    "        results = retriever.invoke(encoded_dog_image)\n",
    "        cats_retrieved = 0\n",
    "        for i, result in enumerate(results):\n",
    "            if 'cat' in result.metadata['image_path']:\n",
    "                cats_retrieved += (4 - i)\n",
    "        cat_scores[dog_path] = cats_retrieved\n",
    "    most_similar_dog = max(cat_scores, key=cat_scores.get)\n",
    "    return most_similar_dog, cat_scores[most_similar_dog]\n",
    "\n",
    "# Find the dog most similar to cats\n",
    "most_similar_dog, score = find_most_similar_dog(dog_image_paths, retriever)\n",
    "\n",
    "# Print the result\n",
    "print(f\"The dog most similar to cats is: {most_similar_dog} with a score of {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 3: Multimodality\n",
    "\n",
    "### What is Multimodality?\n",
    "\n",
    "Multimodal AI applications deal with multiple types of data. The term comes from \"multi\" (many) and \"modal\" (types). Multimodal AI mimics human senses (sight, hearing, taste, touch, smell). The core idea behind multimodal AI applications is that they deal with multiple types of data. Multimodal AI is popular because it gives AI more human-like power. Humans have a multimodal interface with the world through senses like sight, hearing, taste, touch, and smell. In AI, the most emulated modalities are sight and hearing. While the term \"multimodal\" is debated, examples include images and text, images and audio, and video. These correspond to human senses like sight and sound. Machines process images and texts as different kinds of data, even though both relate to sight. Some multimodal data examples are easier to relate to human senses, while others are more contested. Examples include PDFs and CSVs, texts and tables, and tables and graphs. These examples fall into the sight category but need different machine learning models for processing.\n",
    "\n",
    "- **Common Modalities in AI**:\n",
    "  - Images and text\n",
    "  - Images and audio\n",
    "  - Video\n",
    "  - PDFs and CSVs\n",
    "  - Texts and tables\n",
    "  - Tables and graphs\n",
    "\n",
    "### 3.1. Introduction to Multimodal Embedding Models\n",
    "\n",
    "Sure, here are the models with additional information for GPT-4o and LLaVa:\n",
    "\n",
    "1. **CLIP (Contrastive Language-Image Pretraining)**:\n",
    "   - Two encoders: one for images, one for text.\n",
    "   - Trained on 400 million image-text pairs.\n",
    "   - Contrastive learning aligns two modalities.\n",
    "   - Encoders represent pairs closely and unpaired combos far apart.\n",
    "   - Same embedding space and dimensionality for vectors.\n",
    "\n",
    "2. **GPT-4o**:\n",
    "   - Developed by OpenAI, GPT-4o (\"o\" for \"omni\") is a multilingual, multimodal generative pre-trained transformer.\n",
    "   - Released in May 2024, it can process and generate text, images, and audio.\n",
    "   - Achieves GPT-4 Turbo-level performance on text, reasoning, and coding intelligence, with significant improvements in multilingual, audio, and vision capabilities[1](https://openai.com/index/hello-gpt-4o/)[2](https://en.wikipedia.org/wiki/GPT-4o).\n",
    "   - Supports over 50 languages and has a context length of 128k tokens[2](https://en.wikipedia.org/wiki/GPT-4o).\n",
    "   - Designed for real-time interaction, it can respond to audio inputs in as little as 232 milliseconds[1](https://openai.com/index/hello-gpt-4o/).\n",
    "   - Offers advanced voice-to-voice capabilities and real-time translation[2](https://en.wikipedia.org/wiki/GPT-4o).\n",
    "\n",
    "3. **LLaVa**:\n",
    "   - LLaVa (Large Language and Vision Assistant) is an open-source multimodal model.\n",
    "   - Combines an image encoder with a large language model (LLM), fine-tuned on multimodal instruction-following data[3](https://huggingface.co/docs/transformers/main/model_doc/llava).\n",
    "   - Developed by fine-tuning LLaMA/Vicuna on GPT-generated data, it achieves state-of-the-art performance across multiple benchmarks[3](https://huggingface.co/docs/transformers/main/model_doc/llava)[4](https://github.com/haotian-liu/LLaVA).\n",
    "   - Uses a fully-connected vision-language cross-modal connector, making it powerful and data-efficient[3](https://huggingface.co/docs/transformers/main/model_doc/llava).\n",
    "   - Capable of processing and generating text and images, and designed for tasks like visual question answering and image captioning[4](https://github.com/haotian-liu/LLaVA).\n",
    "\n",
    "4. **DALL-E**:\n",
    "   - Developed by OpenAI, DALL-E generates images from textual descriptions.\n",
    "   - Uses a transformer model to understand and generate images based on text prompts.\n",
    "   - Trained on a large dataset of text-image pairs.\n",
    "\n",
    "5. **ALIGN (A Large-scale ImaGe and Noisy-text embedding)**:\n",
    "   - Developed by Google, ALIGN uses a dual-encoder architecture similar to CLIP.\n",
    "   - Trained on a large dataset of noisy image-text pairs from the web.\n",
    "   - Aligns visual and textual representations in a shared embedding space.\n",
    "\n",
    "6. **Florence**:\n",
    "   - Developed by Microsoft, Florence is a unified image-text model.\n",
    "   - Uses a transformer-based architecture to process and align images and text.\n",
    "   - Trained on a diverse set of image-text pairs to perform various vision-language tasks.\n",
    "\n",
    "7. **BLIP (Bootstrapping Language-Image Pre-training)**:\n",
    "   - Developed by Salesforce, BLIP uses a bootstrapping approach to improve image-text alignment.\n",
    "   - Trained on a large dataset of image-text pairs with a focus on improving the quality of generated captions and image retrieval.\n",
    "\n",
    "### 3.2. Ways to Do Multimodal RAG\n",
    "\n",
    "1. **Single Multimodal Model**: Handles multiple types of data.\n",
    "   - Uses one multimodal embedding model to process multiple types of data, usually images and text.\n",
    "   - Advantages: One vector store, one embedding model, consistent dimensionality for embeddings.\n",
    "   - Example: CLIP model with image and text options.\n",
    "   - Frameworks like LangChain or LlamaIndex can handle this for you.\n",
    "\n",
    "2. **Multiple Models**:\n",
    "   - Uses multiple embedding models and multiple search modes for different types of data.\n",
    "   - Requires routing each type of data to the right model for storing and searching.\n",
    "   - Separate vector stores for each model.\n",
    "   - Vectors of the same size can be compared, but different models generate embeddings of different lengths.\n",
    "   - Manual process for routing data.\n",
    "\n",
    "3. **Combination Method**:\n",
    "   - Uses multiple multimodal models.\n",
    "   - Rarely used in practice.\n",
    "   - Reasons to use: Handling many different types of data or re-ranking vector results.\n",
    "   - Routes data to different models and vector stores based on data type.\n",
    "   - Uses tagging system for re-ranking.\n",
    "\n",
    "### 3.3. Embedding and Storing Data\n",
    "\n",
    "**Review**:\n",
    "- Using LangChain to get OpenCLIPEmbeddings.\n",
    "- Storing vectors into FAISS.\n",
    "\n",
    "**Process**:\n",
    "1. Grab images and encode them into Base64 for the LLM.\n",
    "\n",
    "  ```python\n",
    "  # Function to encode image to Base64 string\n",
    "  def encode_image(path):\n",
    "      with open(path, \"rb\") as image_file:\n",
    "          return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "  ```\n",
    "\n",
    "2. Create documents from images.\n",
    "\n",
    "  ```python\n",
    "  # Get all image paths\n",
    "  paths = glob.glob('../images/*.jpeg', recursive=True)\n",
    "\n",
    "  # Create a list of documents with encoded images\n",
    "  lc_docs = []\n",
    "  for path in paths:\n",
    "      doc = Document(\n",
    "          page_content=encode_image(path),\n",
    "          metadata={'source': path}\n",
    "      )\n",
    "      lc_docs.append(doc)\n",
    "  ```\n",
    "\n",
    "3. Use OpenCLIPEmbeddings with documents.\n",
    "\n",
    "  ```python\n",
    "  # Create a vector store from documents using OpenCLIPEmbeddings\n",
    "  vector_store = FAISS.from_documents(lc_docs, embedding=OpenCLIPEmbeddings())\n",
    "  ```\n",
    "\n",
    "  4. Store embeddings into the FAISS vector database.\n",
    "\n",
    "  ```python\n",
    "  retriever = vector_store.as_retriever()\n",
    "  ```\n",
    "\n",
    "### 3.4. Query Images with Text\n",
    "\n",
    "**Process**:\n",
    "\n",
    "1. **Create a retriever object from the vector store**:\n",
    "   - This step involves initializing a retriever from the vector store. The retriever is responsible for querying the vector store to find and retrieve relevant vectors (e.g., images or documents) based on similarity or specific criteria.\n",
    "\n",
    "  ```python\n",
    "  retriever = vector_store.as_retriever()\n",
    "  ```\n",
    "\n",
    "2. **Import `BytesIO` and `Images` for handling byte and image data**:\n",
    "   - These imports are necessary for handling image data in memory. `BytesIO` allows you to work with image data as byte streams, and `PIL.Image` provides image processing capabilities.\n",
    "\n",
    "  ```python\n",
    "  from io import BytesIO\n",
    "  from PIL import Image\n",
    "  ```\n",
    "\n",
    "3. **Create three functions**:\n",
    "   - **Resizing Function**: This function takes a Base64 string, resizes the image, and returns it as a Base64-encoded string. This is useful for ensuring images are of a manageable size for processing and transmission.\n",
    "\n",
    "  ```python\n",
    "  # Function to resize Base64 image\n",
    "  def resize_base64_image(base64_string, size=(128, 128)):\n",
    "      # Decode the Base64 string\n",
    "      img_data = base64.b64decode(base64_string)\n",
    "      img = Image.open(BytesIO(img_data))\n",
    "\n",
    "      # Resize the image\n",
    "      resized_img = img.resize(size, Image.LANCZOS)\n",
    "\n",
    "      # Save the resized image to a bytes buffer\n",
    "      buffered = BytesIO()\n",
    "      resized_img.save(buffered, format=img.format)\n",
    "\n",
    "      # Encode the resized image to Base64\n",
    "      return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "  ```\n",
    "\n",
    "   - **Base64 Check Function**: This function checks if a string is in Base64 format. It ensures that the data being processed is correctly encoded.\n",
    "\n",
    "  ```python\n",
    "  # Function to check if a string is Base64 encoded\n",
    "  def is_base64(s):\n",
    "      try:\n",
    "          return base64.b64encode(base64.b64decode(s)) == s.encode()\n",
    "      except Exception:\n",
    "          return False\n",
    "  ```\n",
    "\n",
    "   - **Split Function**: This function splits image and text input, resizing images if necessary. It separates the data into images and texts for further processing.\n",
    "\n",
    "  ```python\n",
    "  # Function to split image and text types, resizing images if necessary\n",
    "  def split_image_text_types(docs):\n",
    "      images = []\n",
    "      text = []\n",
    "      for doc in docs:\n",
    "          doc_content = doc.page_content  # Extract Document contents\n",
    "          if is_base64(doc_content):\n",
    "              # Resize image to avoid OAI server error\n",
    "              images.append(resize_base64_image(doc_content))  # base64 encoded str\n",
    "          else:\n",
    "              text.append(doc_content)\n",
    "      return {\"images\": images, \"texts\": text}\n",
    "  ```\n",
    "\n",
    "4. **Create a prompt for the multimodal RAG app using imports like `HumanMessage`, `RunnableLambda`, and `ChatOpenAI`**:\n",
    "   - This step involves setting up the necessary imports and creating a function to format the data into prompts that the multimodal RAG app can understand and process.\n",
    "\n",
    "```python\n",
    "from langchain.prompts import HumanMessage\n",
    "from langchain.output_parsers import StrOutputParser\n",
    "from langchain.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "```\n",
    "\n",
    "5. **Make a function to split image and text data and format them into prompts**:\n",
    "   - This function formats the input data (images and text) into a structured prompt that can be used by the language model to generate responses.\n",
    "\n",
    "  ```python\n",
    "  # Function to create prompt for the multimodal RAG app\n",
    "  def prompt_func(data_dict):\n",
    "      # Joining the context texts into a single string\n",
    "      formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
    "      messages = []\n",
    "\n",
    "      # Adding image(s) to the messages if present\n",
    "      if data_dict[\"context\"][\"images\"]:\n",
    "          image_message = {\n",
    "              \"type\": \"image_url\",\n",
    "              \"image_url\": {\n",
    "                  \"url\": f\"data:image/jpeg;base64,{data_dict['context']['images'][0]}\"\n",
    "              },\n",
    "          }\n",
    "          messages.append(image_message)\n",
    "\n",
    "      # Adding the text message for analysis\n",
    "      text_message = {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": (\n",
    "              \"As an animal lover, your task is to analyze and interpret images of cute animals. \"\n",
    "              \"Please use your extensive knowledge and analytical skills to provide a \"\n",
    "              \"summary that includes:\\n\"\n",
    "              \"- A detailed description of the visual elements in the image.\\n\"\n",
    "              f\"User-provided keywords: {data_dict['question']}\\n\\n\"\n",
    "              \"Text and/or tables:\\n\"\n",
    "              f\"{formatted_texts}\"\n",
    "          ),\n",
    "      }\n",
    "      messages.append(text_message)\n",
    "\n",
    "      return [HumanMessage(content=messages)]\n",
    "  ```\n",
    "\n",
    "6. **Use the foundational model (GPT-4o mini) and create a chain**:\n",
    "   - This step involves initializing the foundational model and creating a processing chain that combines the retriever, prompt function, and the language model to handle the input data and generate responses.\n",
    "\n",
    "  ```python\n",
    "  foundation = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\", max_tokens=1024)\n",
    "\n",
    "  # RAG pipeline\n",
    "  chain = (\n",
    "      {\n",
    "          \"context\": retriever | RunnableLambda(split_image_text_types),\n",
    "          \"question\": RunnablePassthrough(),\n",
    "      }\n",
    "      | RunnableLambda(prompt_func)\n",
    "      | foundation\n",
    "      | StrOutputParser()\n",
    "  )\n",
    "  ```\n",
    "\n",
    "7. **Invoke the chain to query images with text (e.g., looking for a rottweiler in images)**:\n",
    "   - This step demonstrates how to use the created chain to query the vector store with a combination of image and text data, retrieving and processing the most relevant results.\n",
    "\n",
    "  ```python\n",
    "  # Example usage: Querying for a rottweiler in images with context and question\n",
    "  result = chain.invoke({\n",
    "      \"docs\": [\"<Base64_encoded_image_string>\", \"rottweiler\"],\n",
    "      \"context\": {\"texts\": [\"This is a search for dog breeds.\"], \"images\": []},\n",
    "      \"question\": \"Find images of a rottweiler\"\n",
    "  })\n",
    "\n",
    "  # Output the result\n",
    "  print(result)\n",
    "\n",
    "  # Retrieve documents based on text query \"rottweiler\"\n",
    "  docs = retriever.invoke(\"rottweiler\", k=4)  # dog 5\n",
    "  for doc in docs:\n",
    "      print(doc.metadata)\n",
    "\n",
    "  # Retrieve documents based on encoded image of cat_1.jpeg\n",
    "  docs = retriever.invoke(encode_image(\"../images/cat_1.jpeg\"), k=4)  # cat 1\n",
    "  for doc in docs:\n",
    "      print(doc.metadata)\n",
    "\n",
    "  # Retrieve documents based on text query \"gray cat with long hair in a field\"\n",
    "  docs = retriever.invoke(\"gray cat with long hair in a field\", k=4)  # cat 2\n",
    "  for doc in docs:\n",
    "      print(doc.metadata)\n",
    "\n",
    "  # Retrieve documents based on text query \"golden retriever playing with orange ball\"\n",
    "  docs = retriever.invoke(\"golden retriever playing with orange ball\", k=4)  # dog 2\n",
    "  for doc in docs:\n",
    "      print(doc.metadata)\n",
    "\n",
    "  # Retrieve documents based on text query \"golden retriever in field with a sunny blurred background\"\n",
    "  docs = retriever.invoke(\"golden retriever in field with a sunny blurred background\", k=4)  # dog 4\n",
    "  for doc in docs:\n",
    "      print(doc.metadata)\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_experimental.open_clip import OpenCLIPEmbeddings\n",
    "import glob\n",
    "import base64\n",
    "\n",
    "# Function to encode image to Base64 string\n",
    "def encode_image(path):\n",
    "    with open(path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "# Get all image paths\n",
    "paths = glob.glob('../images/*.jpeg', recursive=True)\n",
    "\n",
    "# Create a list of documents with encoded images\n",
    "lc_docs = []\n",
    "for path in paths:\n",
    "    doc = Document(\n",
    "        page_content=encode_image(path),\n",
    "        metadata={\n",
    "            'source': path\n",
    "        }\n",
    "    )\n",
    "    lc_docs.append(doc)\n",
    "\n",
    "# Create a vector store from documents using OpenCLIPEmbeddings\n",
    "vector_store = FAISS.from_documents(lc_docs, embedding=OpenCLIPEmbeddings())\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "# Function to resize Base64 image\n",
    "def resize_base64_image(base64_string, size=(128, 128)):\n",
    "    # Decode the Base64 string\n",
    "    img_data = base64.b64decode(base64_string)\n",
    "    img = Image.open(BytesIO(img_data))\n",
    "\n",
    "    # Resize the image\n",
    "    resized_img = img.resize(size, Image.LANCZOS)\n",
    "\n",
    "    # Save the resized image to a bytes buffer\n",
    "    buffered = BytesIO()\n",
    "    resized_img.save(buffered, format=img.format)\n",
    "\n",
    "    # Encode the resized image to Base64\n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "# Function to check if a string is Base64 encoded\n",
    "def is_base64(s):\n",
    "    try:\n",
    "        return base64.b64encode(base64.b64decode(s)) == s.encode()\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# Function to split image and text types, resizing images if necessary\n",
    "def split_image_text_types(docs):\n",
    "    images = []\n",
    "    text = []\n",
    "    for doc in docs:\n",
    "        doc_content = doc.page_content  # Extract Document contents\n",
    "        if is_base64(doc_content):\n",
    "            # Resize image to avoid OAI server error\n",
    "            images.append(resize_base64_image(doc_content))  # base64 encoded str\n",
    "        else:\n",
    "            text.append(doc_content)\n",
    "    return {\"images\": images, \"texts\": text}\n",
    "\n",
    "from langchain.prompts import HumanMessage\n",
    "from langchain.output_parsers import StrOutputParser\n",
    "from langchain.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Function to create prompt for the multimodal RAG app\n",
    "def prompt_func(data_dict):\n",
    "    # Joining the context texts into a single string\n",
    "    formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
    "    messages = []\n",
    "\n",
    "    # Adding image(s) to the messages if present\n",
    "    if data_dict[\"context\"][\"images\"]:\n",
    "        image_message = {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": f\"data:image/jpeg;base64,{data_dict['context']['images'][0]}\"\n",
    "            },\n",
    "        }\n",
    "        messages.append(image_message)\n",
    "\n",
    "    # Adding the text message for analysis\n",
    "    text_message = {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            \"As an animal lover, your task is to analyze and interpret images of cute animals. \"\n",
    "            \"Please use your extensive knowledge and analytical skills to provide a \"\n",
    "            \"summary that includes:\\n\"\n",
    "            \"- A detailed description of the visual elements in the image.\\n\"\n",
    "            f\"User-provided keywords: {data_dict['question']}\\n\\n\"\n",
    "            \"Text and/or tables:\\n\"\n",
    "            f\"{formatted_texts}\"\n",
    "        ),\n",
    "    }\n",
    "    messages.append(text_message)\n",
    "\n",
    "    return [HumanMessage(content=messages)]\n",
    "\n",
    "foundation = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\", max_tokens=1024)\n",
    "\n",
    "# RAG pipeline\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever | RunnableLambda(split_image_text_types),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | RunnableLambda(prompt_func)\n",
    "    | foundation\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Example usage: Querying for a rottweiler in images with context and question\n",
    "result = chain.invoke({\n",
    "    \"docs\": [\"<Base64_encoded_image_string>\", \"rottweiler\"],\n",
    "    \"context\": {\"texts\": [\"This is a search for dog breeds.\"], \"images\": []},\n",
    "    \"question\": \"Find images of a rottweiler\"\n",
    "})\n",
    "\n",
    "# Output the result\n",
    "print(result)\n",
    "\n",
    "# Retrieve documents based on text query \"rottweiler\"\n",
    "docs = retriever.invoke(\"rottweiler\", k=4)  # dog 5\n",
    "for doc in docs:\n",
    "    print(doc.metadata)\n",
    "\n",
    "# Retrieve documents based on encoded image of cat_1.jpeg\n",
    "docs = retriever.invoke(encode_image(\"../images/cat_1.jpeg\"), k=4)  # cat 1\n",
    "for doc in docs:\n",
    "    print(doc.metadata)\n",
    "\n",
    "# Retrieve documents based on text query \"gray cat with long hair in a field\"\n",
    "docs = retriever.invoke(\"gray cat with long hair in a field\", k=4)  # cat 2\n",
    "for doc in docs:\n",
    "    print(doc.metadata)\n",
    "\n",
    "# Retrieve documents based on text query \"golden retriever playing with orange ball\"\n",
    "docs = retriever.invoke(\"golden retriever playing with orange ball\", k=4)  # dog 2\n",
    "for doc in docs:\n",
    "    print(doc.metadata)\n",
    "\n",
    "# Retrieve documents based on text query \"golden retriever in field with a sunny blurred background\"\n",
    "docs = retriever.invoke(\"golden retriever in field with a sunny blurred background\", k=4)  # dog 4\n",
    "for doc in docs:\n",
    "    print(doc.metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_experimental.open_clip import OpenCLIPEmbeddings\n",
    "import glob\n",
    "import base64\n",
    "\n",
    "# Function to encode image to Base64 string\n",
    "def encode_image(path):\n",
    "    with open(path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "# Get all image paths\n",
    "paths = glob.glob('../images/*.jpeg', recursive=True)\n",
    "\n",
    "# Create a list of documents with encoded images\n",
    "lc_docs = []\n",
    "for path in paths:\n",
    "    doc = Document(\n",
    "        page_content=encode_image(path),\n",
    "        metadata={\n",
    "            'source': path\n",
    "        }\n",
    "    )\n",
    "    lc_docs.append(doc)\n",
    "\n",
    "# Create a vector store from documents using OpenCLIPEmbeddings\n",
    "vector_store = FAISS.from_documents(lc_docs, embedding=OpenCLIPEmbeddings())\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "# Function to resize Base64 image\n",
    "def resize_base64_image(base64_string, size=(128, 128)):\n",
    "    # Decode the Base64 string\n",
    "    img_data = base64.b64decode(base64_string)\n",
    "    img = Image.open(BytesIO(img_data))\n",
    "\n",
    "    # Resize the image\n",
    "    resized_img = img.resize(size, Image.LANCZOS)\n",
    "\n",
    "    # Save the resized image to a bytes buffer\n",
    "    buffered = BytesIO()\n",
    "    resized_img.save(buffered, format=img.format)\n",
    "\n",
    "    # Encode the resized image to Base64\n",
    "    return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "# Function to check if a string is Base64 encoded\n",
    "def is_base64(s):\n",
    "    try:\n",
    "        return base64.b64encode(base64.b64decode(s)) == s.encode()\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "# Function to split image and text types, resizing images if necessary\n",
    "def split_image_text_types(docs):\n",
    "    images = []\n",
    "    text = []\n",
    "    for doc in docs:\n",
    "        doc_content = doc.page_content  # Extract Document contents\n",
    "        if is_base64(doc_content):\n",
    "            # Resize image to avoid OAI server error\n",
    "            images.append(resize_base64_image(doc_content))  # base64 encoded str\n",
    "        else:\n",
    "            text.append(doc_content)\n",
    "    return {\"images\": images, \"texts\": text}\n",
    "\n",
    "from langchain.prompts import HumanMessage\n",
    "from langchain.output_parsers import StrOutputParser\n",
    "from langchain.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Function to create prompt for the multimodal RAG app\n",
    "def prompt_func(data_dict):\n",
    "    # Joining the context texts into a single string\n",
    "    formatted_texts = \"\\n\".join(data_dict[\"context\"][\"texts\"])\n",
    "    messages = []\n",
    "\n",
    "    # Adding image(s) to the messages if present\n",
    "    if data_dict[\"context\"][\"images\"]:\n",
    "        image_message = {\n",
    "            \"type\": \"image_url\",\n",
    "            \"image_url\": {\n",
    "                \"url\": f\"data:image/jpeg;base64,{data_dict['context']['images'][0]}\"\n",
    "            },\n",
    "        }\n",
    "        messages.append(image_message)\n",
    "\n",
    "    # Adding the text message for analysis\n",
    "    text_message = {\n",
    "        \"type\": \"text\",\n",
    "        \"text\": (\n",
    "            \"As an animal lover, your task is to analyze and interpret images of cute animals. \"\n",
    "            \"Please use your extensive knowledge and analytical skills to provide a \"\n",
    "            \"summary that includes:\\n\"\n",
    "            \"- A detailed description of the visual elements in the image.\\n\"\n",
    "            f\"User-provided keywords: {data_dict['question']}\\n\\n\"\n",
    "            \"Text and/or tables:\\n\"\n",
    "            f\"{formatted_texts}\"\n",
    "        ),\n",
    "    }\n",
    "    messages.append(text_message)\n",
    "\n",
    "    return [HumanMessage(content=messages)]\n",
    "\n",
    "foundation = ChatOpenAI(temperature=0, model=\"gpt-4o-mini\", max_tokens=1024)\n",
    "\n",
    "# RAG pipeline\n",
    "chain = (\n",
    "    {\n",
    "        \"context\": retriever | RunnableLambda(split_image_text_types),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | RunnableLambda(prompt_func)\n",
    "    | foundation\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Example usage: Querying for a rottweiler in images with context and question\n",
    "result = chain.invoke({\n",
    "    \"docs\": [\"<Base64_encoded_image_string>\", \"rottweiler\"],\n",
    "    \"context\": {\"texts\": [\"This is a search for dog breeds.\"], \"images\": []},\n",
    "    \"question\": \"Find images of a rottweiler\"\n",
    "})\n",
    "\n",
    "# Output the result\n",
    "print(result)\n",
    "\n",
    "# Retrieve documents based on text query \"rottweiler\"\n",
    "docs = retriever.invoke(\"rottweiler\", k=4)  # dog 5\n",
    "for doc in docs:\n",
    "    print(doc.metadata)\n",
    "\n",
    "# Retrieve documents based on encoded image of cat_1.jpeg\n",
    "docs = retriever.invoke(encode_image(\"../images/cat_1.jpeg\"), k=4)  # cat 1\n",
    "for doc in docs:\n",
    "    print(doc.metadata)\n",
    "\n",
    "# Retrieve documents based on text query \"gray cat with long hair in a field\"\n",
    "docs = retriever.invoke(\"gray cat with long hair in a field\", k=4)  # cat 2\n",
    "for doc in docs:\n",
    "    print(doc.metadata)\n",
    "\n",
    "# Retrieve documents based on text query \"golden retriever playing with orange ball\"\n",
    "docs = retriever.invoke(\"golden retriever playing with orange ball\", k=4)  # dog 2\n",
    "for doc in docs:\n",
    "    print(doc.metadata)\n",
    "\n",
    "# Retrieve documents based on text query \"golden retriever in field with a sunny blurred background\"\n",
    "docs = retriever.invoke(\"golden retriever in field with a sunny blurred background\", k=4)  # dog 4\n",
    "for doc in docs:\n",
    "    print(doc.metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "How can you use LangChain to encode images, create a vector store, and retrieve similar documents based on text queries and encoded images?\n",
    "\n",
    "### Solution Explanation:\n",
    "1. **Imports**: Corrected the import statements to use `langchain` instead of `langchain_core`.\n",
    "   ```python\n",
    "   from langchain import Document, FAISS, OpenCLIPEmbeddings\n",
    "   import glob\n",
    "   import base64\n",
    "   ```\n",
    "\n",
    "2. **Encoding Function**: Encodes images to Base64 strings.\n",
    "   ```python\n",
    "   def encode_image(path):\n",
    "       with open(path, \"rb\") as image_file:\n",
    "           return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "   ```\n",
    "\n",
    "3. **Document Creation**: Creates documents with encoded images and metadata.\n",
    "   ```python\n",
    "   paths = glob.glob('../images/*.jpeg', recursive=True)\n",
    "   lc_docs = []\n",
    "   for path in paths:\n",
    "       doc = Document(\n",
    "           page_content=encode_image(path),\n",
    "           metadata={'source': path}\n",
    "       )\n",
    "       lc_docs.append(doc)\n",
    "   ```\n",
    "\n",
    "4. **Vector Store**: Creates a vector store from the documents using `OpenCLIPEmbeddings`.\n",
    "   ```python\n",
    "   vector_store = FAISS.from_documents(lc_docs, embedding=OpenCLIPEmbeddings())\n",
    "   ```\n",
    "\n",
    "5. **Retriever**: Initializes a retriever from the vector store.\n",
    "   ```python\n",
    "   retriever = vector_store.as_retriever()\n",
    "   ```\n",
    "\n",
    "6. **Queries**: Retrieves documents based on various text queries and encoded images, printing the metadata of the results.\n",
    "   ```python\n",
    "   # Retrieve documents based on text query \"rottweiler\"\n",
    "   docs = retriever.invoke(\"rottweiler\", k=4)\n",
    "   for doc in docs:\n",
    "       print(doc.metadata)\n",
    "\n",
    "   # Retrieve documents based on encoded image of cat_1.jpeg\n",
    "   docs = retriever.invoke(encode_image(\"../images/cat_1.jpeg\"), k=4)\n",
    "   for doc in docs:\n",
    "       print(doc.metadata)\n",
    "\n",
    "   # Retrieve documents based on text query \"gray cat with long hair in a field\"\n",
    "   docs = retriever.invoke(\"gray cat with long hair in a field\", k=4)\n",
    "   for doc in docs:\n",
    "       print(doc.metadata)\n",
    "\n",
    "   # Retrieve documents based on text query \"golden retriever playing with orange ball\"\n",
    "   docs = retriever.invoke(\"golden retriever playing with orange ball\", k=4)\n",
    "   for doc in docs:\n",
    "       print(doc.metadata)\n",
    "\n",
    "   # Retrieve documents based on text query \"golden retriever in field with a sunny blurred background\"\n",
    "   docs = retriever.invoke(\"golden retriever in field with a sunny blurred background\", k=4)\n",
    "   for doc in docs:\n",
    "       print(doc.metadata)\n",
    "   ```\n",
    "\n",
    "This solution demonstrates how to encode images, create a vector store, and retrieve similar documents using LangChain. The code includes functions for encoding images, creating documents, initializing a vector store, and querying the store to find similar images based on text descriptions or other encoded images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_experimental.open_clip import OpenCLIPEmbeddings\n",
    "import glob\n",
    "import base64\n",
    "\n",
    "# Function to encode image to Base64 string\n",
    "def encode_image(path):\n",
    "    with open(path, \"rb\") as image_file:\n",
    "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
    "\n",
    "# Get all image paths\n",
    "paths = glob.glob('../images/*.jpeg', recursive=True)\n",
    "\n",
    "# Create a list of documents with encoded images\n",
    "lc_docs = []\n",
    "for path in paths:\n",
    "    doc = Document(\n",
    "        page_content=encode_image(path),\n",
    "        metadata={\n",
    "            'source': path\n",
    "        }\n",
    "    )\n",
    "    lc_docs.append(doc)\n",
    "\n",
    "# Create a vector store from documents using OpenCLIPEmbeddings\n",
    "vector_store = FAISS.from_documents(lc_docs, embedding=OpenCLIPEmbeddings())\n",
    "retriever = vector_store.as_retriever()\n",
    "\n",
    "# Retrieve documents based on text query \"rottweiler\"\n",
    "docs = retriever.invoke(\"rottweiler\", k=4)  # dog 5\n",
    "for doc in docs:\n",
    "    print(doc.metadata)\n",
    "\n",
    "# Retrieve documents based on encoded image of cat_1.jpeg\n",
    "docs = retriever.invoke(encode_image(\"../images/cat_1.jpeg\"), k=4)  # cat 1\n",
    "for doc in docs:\n",
    "    print(doc.metadata)\n",
    "\n",
    "# Retrieve documents based on text query \"gray cat with long hair in a field\"\n",
    "docs = retriever.invoke(\"gray cat with long hair in a field\", k=4)  # cat 2\n",
    "for doc in docs:\n",
    "    print(doc.metadata)\n",
    "\n",
    "# Retrieve documents based on text query \"golden retriever playing with orange ball\"\n",
    "docs = retriever.invoke(\"golden retriever playing with orange ball\", k=4)  # dog 2\n",
    "for doc in docs:\n",
    "    print(doc.metadata)\n",
    "\n",
    "# Retrieve documents based on text query \"golden retriever in field with a sunny blurred background\"\n",
    "docs = retriever.invoke(\"golden retriever in field with a sunny blurred background\", k=4)  # dog 4\n",
    "for doc in docs:\n",
    "    print(doc.metadata)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
